{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2a44d37",
   "metadata": {},
   "source": [
    "## Our reformatting functions\n",
    "- Redundant token removal\n",
    "- Example concatenation for longer training sequences\n",
    "- Preposing\n",
    "- Preposing + Interjection\n",
    "- ReCOGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb833a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, random, copy\n",
    "\n",
    "import pandas as pd\n",
    "import re, random, copy\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import collections\n",
    "import unicodedata\n",
    "import torch\n",
    "import six\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "np_re = re.compile(r\"\"\"\n",
    "    ^\n",
    "    \\s*(\\*)?\n",
    "    \\s*(\\w+?)\\s*\n",
    "    \\(\n",
    "    \\s*(.+?)\\s*\n",
    "    \\)\n",
    "    \\s*$\"\"\", re.VERBOSE)\n",
    "\n",
    "pred_re = re.compile(r\"\"\"\n",
    "    ^\n",
    "    \\s*(\\w+?)\\s*\n",
    "    \\.\n",
    "    \\s*(\\w+?)\\s*\n",
    "    \\(\n",
    "    \\s*(.+?)\\s*\n",
    "    ,\n",
    "    \\s*(.+?)\\s*\n",
    "    \\)\n",
    "    \\s*$\"\"\", re.VERBOSE)\n",
    "\n",
    "mod_re = re.compile(r\"\"\"\n",
    "    ^\n",
    "    \\s*(\\w+?)\\s*\n",
    "    \\.\n",
    "    \\s*(\\w+?)\\s*\n",
    "    \\.\n",
    "    \\s*(\\w+?)\\s*\n",
    "    \\(\n",
    "    \\s*(.+?)\\s*\n",
    "    ,\n",
    "    \\s*(.+?)\\s*\n",
    "    \\)\n",
    "    \\s*$\"\"\", re.VERBOSE)\n",
    "\n",
    "\n",
    "def parse_np(phi):   \n",
    "    the, pred, var = np_re.search(phi).groups()\n",
    "    indef = '' if the is None else '*'\n",
    "    return {'type': 'np', 'definiteness': indef, 'pred': pred, 'entvar': var}\n",
    "\n",
    "def parse_pred(phi):\n",
    "    pred, role, eventvar, entvar = pred_re.search(phi).groups()\n",
    "    return {'type': 'role', 'role': role, 'pred': pred, 'entvar': entvar, 'eventvar': eventvar}\n",
    "\n",
    "def parse_mod(phi):\n",
    "    nppred, rel, pred, e1, e2 = mod_re.search(phi).groups()\n",
    "    # Keeping `rel` even though it is always 'nmod'\n",
    "    return {'type': 'mod', 'rel': rel, 'pred': pred, 'nppred': nppred, 'e1': e1, 'e2': e2}\n",
    "\n",
    "def translate_entity_simplied(entvar, data):\n",
    "    ent = [e for e in data if e['type'] == 'np' and e.get(\"entvar\") == entvar]\n",
    "    if not ent:\n",
    "        return entvar, entvar\n",
    "    else:\n",
    "        ent = ent[0]\n",
    "        return f\"{ent['definiteness']} {ent['entvar']} ( {ent['pred']} )\", f\"{ent['definiteness']} {ent['pred']}\"\n",
    "\n",
    "def translate_entity(entvar, data):\n",
    "    ent = [e for e in data if e['type'] == 'np' and e.get(\"entvar\") == entvar]\n",
    "    if not ent:\n",
    "        return entvar\n",
    "    else:\n",
    "        ent = ent[0]\n",
    "        return f\"{ent['definiteness']} {ent['entvar']} ( {ent['pred']} )\"\n",
    "    \n",
    "def convert_to_unicode(text):\n",
    "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "    if six.PY3:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, bytes):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    elif six.PY2:\n",
    "        if isinstance(text, str):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        elif isinstance(text, unicode):\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    else:\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")\n",
    "\n",
    "def convert_tokens_to_ids(vocab, tokens):\n",
    "    \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n",
    "    ids = []\n",
    "    for token in tokens:\n",
    "        if token not in vocab.keys():\n",
    "            ids.append(vocab['[UNK]'])\n",
    "        else:\n",
    "            ids.append(vocab[token])\n",
    "    return ids\n",
    "        \n",
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = collections.OrderedDict()\n",
    "    index = 0\n",
    "    with open(vocab_file, \"r\") as reader:\n",
    "        while True:\n",
    "            token = convert_to_unicode(reader.readline())\n",
    "            if not token:\n",
    "                break\n",
    "            token = token.strip()\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c03764",
   "metadata": {},
   "source": [
    "### Redundant Token Removal\n",
    "TODO: currently, we just have to change the removing set of tokens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dda3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_removal(text, phi): \n",
    "    global all_roles\n",
    "    removing_set = {'x', '_', '(', ')', ','}\n",
    "    # Parsing:\n",
    "    terms = []\n",
    "    for t in phi.split():\n",
    "        if t not in removing_set:\n",
    "            terms += [t]\n",
    "    ret = \" \".join(terms).strip()\n",
    "    return ret\n",
    "train_df = pd.read_csv(\"./cogs/train.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "dev_df = pd.read_csv(\"./cogs/dev.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "test_df = pd.read_csv(\"./cogs/test.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "gen_df = pd.read_csv(\"./cogs/gen.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "\n",
    "train_df['LF'] = train_df[['sentence', 'LF']].apply(lambda x: token_removal(*x), axis=1,)\n",
    "dev_df['LF'] = dev_df[['sentence', 'LF']].apply(lambda x: token_removal(*x), axis=1,)\n",
    "test_df['LF'] = test_df[['sentence', 'LF']].apply(lambda x: token_removal(*x), axis=1,)\n",
    "gen_df['LF'] = gen_df[['sentence', 'LF']].apply(lambda x: token_removal(*x), axis=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746c3bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_postfix = \"remove_x_(,)\"\n",
    "train_df.to_csv(f'./cogs_token_removal/train_{dataset_postfix}.tsv', sep='\\t', index=False, header=False)\n",
    "dev_df.to_csv(f'./cogs_token_removal/dev_{dataset_postfix}.tsv', sep='\\t', index=False, header=False)\n",
    "test_df.to_csv(f'./cogs_token_removal/test_{dataset_postfix}.tsv', sep='\\t', index=False, header=False)\n",
    "gen_df.to_csv(f'./cogs_token_removal/gen_{dataset_postfix}.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c1d099",
   "metadata": {},
   "source": [
    "### Example Concatenations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cabc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex(LFs, initial_indexes):\n",
    "    new_LF_prefix = []\n",
    "    new_LF_body = []\n",
    "    for i in range(len(LFs)):\n",
    "        if initial_indexes[i] != 0:\n",
    "            new_lf = []\n",
    "            for item in LFs[i].split():\n",
    "                if item.isnumeric():\n",
    "                    new_i = int(item) + initial_indexes[i]\n",
    "                    new_lf += [str(new_i)]\n",
    "                else:\n",
    "                    new_lf += [item]\n",
    "            new_lf = \" \".join(new_lf)\n",
    "        else:\n",
    "            new_lf = LFs[i]\n",
    "        \n",
    "        for item in new_lf.split(\" ; \"):\n",
    "            if \"*\" in item:\n",
    "                new_LF_prefix += [item]\n",
    "            else:\n",
    "                new_LF_body += [item]\n",
    "        new_LF_body += [\"AND\"]\n",
    "    new_LF_body = new_LF_body[:-1]\n",
    "    return \" ; \".join(new_LF_prefix) + \" ; \" + \" \".join(new_LF_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7785a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_ks = [256, 512, 1024, 2048, 3072]\n",
    "for append_k in append_ks:\n",
    "    train_df = pd.read_csv(\"./cogs/train.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "    train_df_org = train_df.copy()\n",
    "    train_df = train_df[train_df[\"type\"] != \"primitive\"]\n",
    "    dev_df = pd.read_csv(\"./cogs/dev.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "    test_df = pd.read_csv(\"./cogs/test.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "    gen_df = pd.read_csv(\"./cogs/gen.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "    dataset_postfix = f\"k_{append_k}\"\n",
    "    append_data = []\n",
    "    start_indexes = [i*6 for i in range(append_k)]\n",
    "    sorted_train_df = train_df.sort_values(by=\"sentence\", key=lambda x: x.str.len())\n",
    "    for start_index in start_indexes:\n",
    "        append_data += [\n",
    "            [sorted_train_df.iloc[-1-start_index].sentence[:-1]+\\\n",
    "            sorted_train_df.iloc[-2-start_index].sentence[0].lower()+\\\n",
    "            sorted_train_df.iloc[-2-start_index].sentence[1:-1]+\\\n",
    "            sorted_train_df.iloc[-3-start_index].sentence[0].lower()+\\\n",
    "            sorted_train_df.iloc[-3-start_index].sentence[1:],\n",
    "            reindex(\n",
    "                [\n",
    "                    sorted_train_df.iloc[-1-start_index].LF,\n",
    "                    sorted_train_df.iloc[-2-start_index].LF,\n",
    "                    sorted_train_df.iloc[-3-start_index].LF\n",
    "                ],\n",
    "                [\n",
    "                    0,\n",
    "                    len(sorted_train_df.iloc[-1-start_index].sentence[:-1].strip().split()),\n",
    "                    len(sorted_train_df.iloc[-1-start_index].sentence[:-1].strip().split())+\n",
    "                    len(sorted_train_df.iloc[-2-start_index].sentence[:-1].strip().split())\n",
    "                ]\n",
    "            ),\n",
    "            'concat']\n",
    "        ]\n",
    "    append_df = pd.DataFrame(append_data, columns =['sentence', 'LF', 'type'])\n",
    "    train_df = pd.concat([train_df_org, append_df])\n",
    "    train_df.to_csv(f'./cogs_concat/train_{dataset_postfix}.tsv', sep='\\t', index=False, header=False)\n",
    "    dev_df.to_csv(f'./cogs_concat/dev_{dataset_postfix}.tsv', sep='\\t', index=False, header=False)\n",
    "    test_df.to_csv(f'./cogs_concat/test_{dataset_postfix}.tsv', sep='\\t', index=False, header=False)\n",
    "    gen_df.to_csv(f'./cogs_concat/gen_{dataset_postfix}.tsv', sep='\\t', index=False, header=False)\n",
    "    \n",
    "    max_s = max(train_df['sentence'].str.split().apply(len))\n",
    "    max_lf = max(train_df['LF'].str.split().apply(len))\n",
    "    print(max_s, max_lf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed5eb6f",
   "metadata": {},
   "source": [
    "### Preposing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845703e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_regular(text, phi, _type):\n",
    "    \n",
    "    if len(phi.split()) == 1:\n",
    "        return text, phi, _type\n",
    "    elif \"LAMBDA\" in phi:\n",
    "        return text, phi, _type\n",
    "    \n",
    "    # parse\n",
    "    text_split = text.split()\n",
    "    data = []    \n",
    "    conjs = re.split(r\"\\s*(?:AND|;)\\s*\", phi)\n",
    "    for conj in conjs: \n",
    "        if np_re.search(conj):\n",
    "            d = parse_np(conj)\n",
    "        elif pred_re.search(conj):\n",
    "            d = parse_pred(conj)\n",
    "        elif mod_re.search(conj):\n",
    "            d = parse_mod(conj)\n",
    "        else:\n",
    "            raise Exception(f\"Conjunct could not be parsed: {conj}\")\n",
    "        data.append(d)\n",
    "    \n",
    "    # collect\n",
    "    def_terms = []\n",
    "    role_terms = []\n",
    "    nmod_terms = []\n",
    "    for d in data:\n",
    "        if d['type'] == 'np':\n",
    "            if d['definiteness'] == '*':\n",
    "                def_terms += [f\"* {d['pred']} ( {d['entvar']} )\"]\n",
    "            else:\n",
    "                role_terms += [f\"{d['pred']} ( {d['entvar']} )\"]\n",
    "        if d['type'] == 'role':\n",
    "            role_terms += [f\"{d['pred']} . {d['role']} ( {d['eventvar']} , {d['entvar']} )\"]\n",
    "        elif d['type'] == 'mod':\n",
    "            assert \"x _\" in d['e1']\n",
    "            role_terms += [f\"{d['nppred']} . nmod . {d['pred']} ( {d['e1']} , {d['e2']} )\"]\n",
    "            \n",
    "    # sort def_terms\n",
    "    def_terms = [*set(def_terms)]\n",
    "    def_terms.sort(key = lambda x: int(x.split()[-2]))    \n",
    "    rest_terms = role_terms\n",
    "    \n",
    "    # combine\n",
    "    def_terms = \" ; \".join(def_terms)\n",
    "    if def_terms == \"\":\n",
    "        terms = \" AND \".join(rest_terms)\n",
    "    elif \" AND \".join(rest_terms) == \"\":\n",
    "        terms = def_terms\n",
    "    else:\n",
    "        terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "\n",
    "    return text, terms, _type\n",
    "        \n",
    "def translate(text, phi, _type):\n",
    "    \n",
    "    if len(phi.split()) == 1:\n",
    "        return text, phi, _type\n",
    "    elif \"LAMBDA\" in phi:\n",
    "        return text, phi, _type\n",
    "    \n",
    "    # parse\n",
    "    text_split = text.split()\n",
    "    data = []    \n",
    "    conjs = re.split(r\"\\s*(?:AND|;)\\s*\", phi)\n",
    "    for conj in conjs: \n",
    "        if np_re.search(conj):\n",
    "            d = parse_np(conj)\n",
    "        elif pred_re.search(conj):\n",
    "            d = parse_pred(conj)\n",
    "        elif mod_re.search(conj):\n",
    "            d = parse_mod(conj)\n",
    "        else:\n",
    "            raise Exception(f\"Conjunct could not be parsed: {conj}\")\n",
    "        data.append(d)\n",
    "    \n",
    "    # collect\n",
    "    def_terms = []\n",
    "    role_terms = []\n",
    "    nmod_terms = []\n",
    "    for d in data:\n",
    "        if d['type'] == 'np':\n",
    "            if d['definiteness'] == '*':\n",
    "                def_terms += [f\"* {d['pred']} ( {d['entvar']} )\"]\n",
    "            else:\n",
    "                role_terms += [f\"{d['pred']} ( {d['entvar']} )\"]\n",
    "        if d['type'] == 'role':\n",
    "            role_terms += [f\"{d['pred']} . {d['role']} ( {d['eventvar']} , {d['entvar']} )\"]\n",
    "        elif d['type'] == 'mod':\n",
    "            assert \"x _\" in d['e1']\n",
    "            role_terms += [f\"{d['nppred']} . nmod . {d['pred']} ( {d['e1']} , {d['e2']} )\"]\n",
    "            \n",
    "    # sort def_terms\n",
    "    def_terms = [*set(def_terms)]\n",
    "    def_terms.sort(key = lambda x: int(x.split()[-2]))    \n",
    "    rest_terms = role_terms\n",
    "    \n",
    "    # combine\n",
    "    def_terms = \" ; \".join(def_terms)\n",
    "    if def_terms == \"\":\n",
    "        terms = \" AND \".join(rest_terms)\n",
    "    elif \" AND \".join(rest_terms) == \"\":\n",
    "        terms = def_terms\n",
    "    else:\n",
    "        terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "    \n",
    "    if \"nmod\" not in terms.split():\n",
    "        return text, terms, _type\n",
    "    \n",
    "    if random.random() >= proposing_prob:\n",
    "        return text, terms, _type\n",
    "    \n",
    "    if terms.split().count(\"nmod\") == 2:\n",
    "        upper_bound = 6\n",
    "        return_type = \"preposing_2\"\n",
    "    elif terms.split().count(\"nmod\") == 1:\n",
    "        upper_bound = 3\n",
    "        return_type = \"preposing_1\"\n",
    "    else:\n",
    "        assert False\n",
    "        \n",
    "    nmod = terms.split()[terms.split().index(\"nmod\")+2]\n",
    "    pre_phrase = text.split()[text.split().index(nmod)-2 : text.split().index(nmod)+upper_bound]\n",
    "    pre_phrase[0] = pre_phrase[0].capitalize()\n",
    "    pre_text = text.split()[:text.split().index(nmod)-2]\n",
    "    if pre_text[0] in [\"The\", \"A\"]:\n",
    "        pre_text[0] = pre_text[0].lower()\n",
    "    post_text = text.split()[text.split().index(nmod)+upper_bound:]\n",
    "    pre_text = pre_phrase + pre_text + post_text\n",
    "    pre_text = \" \".join(pre_text)\n",
    "\n",
    "    index_map = {}\n",
    "    idx = 0\n",
    "    for i in range(text.split().index(nmod)-2, text.split().index(nmod)+upper_bound):\n",
    "        index_map[f\"{i}\"] = f\"{idx}\"\n",
    "        idx += 1\n",
    "    for i in range(text.split().index(nmod)-2):\n",
    "        ii = 2+upper_bound+i\n",
    "        index_map[f\"{i}\"] = f\"{ii}\"\n",
    "        idx += 1\n",
    "    for i in range(text.split().index(nmod)+upper_bound, len(text.split())):\n",
    "        ii = idx+(i-(text.split().index(nmod)+upper_bound))\n",
    "        index_map[f\"{i}\"] = f\"{ii}\"  \n",
    "            \n",
    "    # now handle LF\n",
    "    pre_terms = []\n",
    "    for t in terms.split():\n",
    "        if t.isnumeric():\n",
    "            pre_terms += [str(index_map[str(int(t))])]\n",
    "        else:\n",
    "            pre_terms += [t]\n",
    "    pre_terms = \" \".join(pre_terms)\n",
    "\n",
    "    pre_terms_def = pre_terms.split(\" ; \")[:-1]\n",
    "    pre_terms_def.sort(key = lambda x: int(x.split()[-2]))  \n",
    "    pre_terms_role = pre_terms.split(\" ; \")[-1].split(\" AND \")\n",
    "    pre_terms_role.sort(key = lambda x: (int(x.split()[-2]) if x.split()[-5] == \"(\" else int(x.split()[-6]) if x.split()[-2].isnumeric() else int(x.split()[-4]), -1 if x.split()[-5] == \"(\" else int(x.split()[-2]) if x.split()[-2].isnumeric() else pre_text.split().index(x.split()[-2])))  \n",
    "    pre_terms_role = \" AND \".join(pre_terms_role)\n",
    "    pre_terms = \" ; \".join(pre_terms_def + [pre_terms_role])\n",
    "\n",
    "    return pre_text, pre_terms, return_type\n",
    "\n",
    "train_df = pd.read_csv(\"./cogs/train.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "dev_df = pd.read_csv(\"./cogs/dev.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "test_df = pd.read_csv(\"./cogs/test.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "gen_df = pd.read_csv(\"./cogs/gen.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "\n",
    "proposing_prob = 0.05\n",
    "train_df[['sentence', 'LF', 'type']] = train_df[['sentence', 'LF', 'type']].apply(lambda x: translate(*x), axis=1, result_type='expand')\n",
    "dev_df[['sentence', 'LF', 'type']] = dev_df[['sentence', 'LF', 'type']].apply(lambda x: translate_regular(*x), axis=1, result_type='expand')\n",
    "test_df[['sentence', 'LF', 'type']] = test_df[['sentence', 'LF', 'type']].apply(lambda x: translate_regular(*x), axis=1, result_type='expand')\n",
    "gen_df[['sentence', 'LF', 'type']] = gen_df[['sentence', 'LF', 'type']].apply(lambda x: translate_regular(*x), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d26b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_postfix = \"preposing\"\n",
    "train_df.to_csv(f'./cogs_{dataset_postfix}/train.tsv', sep='\\t', index=False, header=False)\n",
    "dev_df.to_csv(f'./cogs_{dataset_postfix}/dev.tsv', sep='\\t', index=False, header=False)\n",
    "test_df.to_csv(f'./cogs_{dataset_postfix}/test.tsv', sep='\\t', index=False, header=False)\n",
    "gen_df.to_csv(f'./cogs_{dataset_postfix}/gen.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96411af9",
   "metadata": {},
   "source": [
    "### Preposing + Sprinkles (Interjection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f5c65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_regular(text, phi, _type):\n",
    "    \n",
    "    if len(phi.split()) == 1:\n",
    "        return text, phi, _type\n",
    "    elif \"LAMBDA\" in phi:\n",
    "        return text, phi, _type\n",
    "    \n",
    "    # parse\n",
    "    text_split = text.split()\n",
    "    data = []    \n",
    "    conjs = re.split(r\"\\s*(?:AND|;)\\s*\", phi)\n",
    "    for conj in conjs: \n",
    "        if np_re.search(conj):\n",
    "            d = parse_np(conj)\n",
    "        elif pred_re.search(conj):\n",
    "            d = parse_pred(conj)\n",
    "        elif mod_re.search(conj):\n",
    "            d = parse_mod(conj)\n",
    "        else:\n",
    "            raise Exception(f\"Conjunct could not be parsed: {conj}\")\n",
    "        data.append(d)\n",
    "    \n",
    "    # collect\n",
    "    def_terms = []\n",
    "    role_terms = []\n",
    "    nmod_terms = []\n",
    "    for d in data:\n",
    "        if d['type'] == 'np':\n",
    "            if d['definiteness'] == '*':\n",
    "                def_terms += [f\"* {d['pred']} ( {d['entvar']} )\"]\n",
    "            else:\n",
    "                role_terms += [f\"{d['pred']} ( {d['entvar']} )\"]\n",
    "        if d['type'] == 'role':\n",
    "            role_terms += [f\"{d['pred']} . {d['role']} ( {d['eventvar']} , {d['entvar']} )\"]\n",
    "        elif d['type'] == 'mod':\n",
    "            assert \"x _\" in d['e1']\n",
    "            role_terms += [f\"{d['nppred']} . nmod . {d['pred']} ( {d['e1']} , {d['e2']} )\"]\n",
    "            \n",
    "    # sort def_terms\n",
    "    def_terms = [*set(def_terms)]\n",
    "    def_terms.sort(key = lambda x: int(x.split()[-2]))    \n",
    "    rest_terms = role_terms\n",
    "    \n",
    "    # combine\n",
    "    def_terms = \" ; \".join(def_terms)\n",
    "    if def_terms == \"\":\n",
    "        terms = \" AND \".join(rest_terms)\n",
    "    elif \" AND \".join(rest_terms) == \"\":\n",
    "        terms = def_terms\n",
    "    else:\n",
    "        terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "\n",
    "    return text, terms, _type\n",
    "        \n",
    "def translate(text, phi, _type):\n",
    "    \n",
    "    if len(phi.split()) == 1:\n",
    "        return text, phi, _type\n",
    "    elif \"LAMBDA\" in phi:\n",
    "        return text, phi, _type\n",
    "    \n",
    "    # parse\n",
    "    text_split = text.split()\n",
    "    data = []    \n",
    "    conjs = re.split(r\"\\s*(?:AND|;)\\s*\", phi)\n",
    "    for conj in conjs: \n",
    "        if np_re.search(conj):\n",
    "            d = parse_np(conj)\n",
    "        elif pred_re.search(conj):\n",
    "            d = parse_pred(conj)\n",
    "        elif mod_re.search(conj):\n",
    "            d = parse_mod(conj)\n",
    "        else:\n",
    "            raise Exception(f\"Conjunct could not be parsed: {conj}\")\n",
    "        data.append(d)\n",
    "    \n",
    "    # collect\n",
    "    def_terms = []\n",
    "    role_terms = []\n",
    "    nmod_terms = []\n",
    "    for d in data:\n",
    "        if d['type'] == 'np':\n",
    "            if d['definiteness'] == '*':\n",
    "                def_terms += [f\"* {d['pred']} ( {d['entvar']} )\"]\n",
    "            else:\n",
    "                role_terms += [f\"{d['pred']} ( {d['entvar']} )\"]\n",
    "        if d['type'] == 'role':\n",
    "            role_terms += [f\"{d['pred']} . {d['role']} ( {d['eventvar']} , {d['entvar']} )\"]\n",
    "        elif d['type'] == 'mod':\n",
    "            assert \"x _\" in d['e1']\n",
    "            role_terms += [f\"{d['nppred']} . nmod . {d['pred']} ( {d['e1']} , {d['e2']} )\"]\n",
    "            \n",
    "    # sort def_terms\n",
    "    def_terms = [*set(def_terms)]\n",
    "    def_terms.sort(key = lambda x: int(x.split()[-2]))    \n",
    "    rest_terms = role_terms\n",
    "    \n",
    "    # combine\n",
    "    def_terms = \" ; \".join(def_terms)\n",
    "    if def_terms == \"\":\n",
    "        terms = \" AND \".join(rest_terms)\n",
    "    elif \" AND \".join(rest_terms) == \"\":\n",
    "        terms = def_terms\n",
    "    else:\n",
    "        terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "    \n",
    "    if \"nmod\" not in terms.split():\n",
    "        return text, terms, _type\n",
    "    \n",
    "    if random.random() >= proposing_prob:\n",
    "        return text, terms, _type\n",
    "    \n",
    "    if terms.split().count(\"nmod\") == 2:\n",
    "        upper_bound = 6\n",
    "        return_type = \"preposing_2\"\n",
    "    elif terms.split().count(\"nmod\") == 1:\n",
    "        upper_bound = 3\n",
    "        return_type = \"preposing_1\"\n",
    "    else:\n",
    "        assert False\n",
    "        \n",
    "    nmod = terms.split()[terms.split().index(\"nmod\")+2]\n",
    "    pre_phrase = text.split()[text.split().index(nmod)-2 : text.split().index(nmod)+upper_bound]\n",
    "    pre_phrase[0] = pre_phrase[0].capitalize()\n",
    "    pre_text = text.split()[:text.split().index(nmod)-2]\n",
    "    if pre_text[0] in [\"The\", \"A\"]:\n",
    "        pre_text[0] = pre_text[0].lower()\n",
    "    post_text = text.split()[text.split().index(nmod)+upper_bound:]\n",
    "    pre_text = pre_phrase + pre_text + post_text\n",
    "    pre_text = \" \".join(pre_text)\n",
    "\n",
    "    index_map = {}\n",
    "    idx = 0\n",
    "    for i in range(text.split().index(nmod)-2, text.split().index(nmod)+upper_bound):\n",
    "        index_map[f\"{i}\"] = f\"{idx}\"\n",
    "        idx += 1\n",
    "    for i in range(text.split().index(nmod)-2):\n",
    "        ii = 2+upper_bound+i\n",
    "        index_map[f\"{i}\"] = f\"{ii}\"\n",
    "        idx += 1\n",
    "    for i in range(text.split().index(nmod)+upper_bound, len(text.split())):\n",
    "        ii = idx+(i-(text.split().index(nmod)+upper_bound))\n",
    "        index_map[f\"{i}\"] = f\"{ii}\"  \n",
    "            \n",
    "    # now handle LF\n",
    "    pre_terms = []\n",
    "    for t in terms.split():\n",
    "        if t.isnumeric():\n",
    "            pre_terms += [str(index_map[str(int(t))])]\n",
    "        else:\n",
    "            pre_terms += [t]\n",
    "    pre_terms = \" \".join(pre_terms)\n",
    "\n",
    "    pre_terms_def = pre_terms.split(\" ; \")[:-1]\n",
    "    pre_terms_def.sort(key = lambda x: int(x.split()[-2]))  \n",
    "    pre_terms_role = pre_terms.split(\" ; \")[-1].split(\" AND \")\n",
    "    pre_terms_role.sort(key = lambda x: (int(x.split()[-2]) if x.split()[-5] == \"(\" else int(x.split()[-6]) if x.split()[-2].isnumeric() else int(x.split()[-4]), -1 if x.split()[-5] == \"(\" else int(x.split()[-2]) if x.split()[-2].isnumeric() else pre_text.split().index(x.split()[-2])))  \n",
    "    pre_terms_role = \" AND \".join(pre_terms_role)\n",
    "    pre_terms = \" ; \".join(pre_terms_def + [pre_terms_role])\n",
    "\n",
    "    return pre_text, pre_terms, return_type\n",
    "\n",
    "def add_um(sentence):\n",
    "    words = sentence.split()\n",
    "    new_words = []\n",
    "    mapping = {}\n",
    "    offset = 0\n",
    "    for i, word in enumerate(words):\n",
    "        mapping[i] = len(new_words)\n",
    "        new_words.append(word)\n",
    "        if i > 0 and i < len(words) - 2 and random.random() > 0.5:\n",
    "            num_um = random.choice([1,2,3])\n",
    "            for j in range(num_um):\n",
    "                new_words.append(\"um\")\n",
    "    return \" \".join(new_words), mapping\n",
    "\n",
    "def sprinkle(text, phi, _type):\n",
    "    if \"preposition\" in _type:\n",
    "        return text, phi, _type\n",
    "    \n",
    "    if random.random() >= sprinkle_prob:\n",
    "        return text, phi, _type\n",
    "    \n",
    "    um_text, token_mapping = add_um(text)\n",
    "    um_phi = []\n",
    "    for t in phi.split():\n",
    "        if t.isnumeric():\n",
    "            um_phi += [str(token_mapping[int(t)])]\n",
    "        else:\n",
    "            um_phi += [t]\n",
    "    um_phi = \" \".join(um_phi)\n",
    "    \n",
    "    return um_text, um_phi, \"sprinkle\"\n",
    "            \n",
    "train_df = pd.read_csv(\"./cogs/train.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "dev_df = pd.read_csv(\"./cogs/dev.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "test_df = pd.read_csv(\"./cogs/test.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "gen_df = pd.read_csv(\"./cogs/gen.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "\n",
    "proposing_prob = 0.05\n",
    "sprinkle_prob = 0.05\n",
    "train_df[['sentence', 'LF', 'type']] = train_df[['sentence', 'LF', 'type']].apply(lambda x: translate(*x), axis=1, result_type='expand')\n",
    "train_df[['sentence', 'LF', 'type']] = train_df[['sentence', 'LF', 'type']].apply(lambda x: sprinkle(*x), axis=1, result_type='expand')\n",
    "dev_df[['sentence', 'LF', 'type']] = dev_df[['sentence', 'LF', 'type']].apply(lambda x: translate_regular(*x), axis=1, result_type='expand')\n",
    "test_df[['sentence', 'LF', 'type']] = test_df[['sentence', 'LF', 'type']].apply(lambda x: translate_regular(*x), axis=1, result_type='expand')\n",
    "gen_df[['sentence', 'LF', 'type']] = gen_df[['sentence', 'LF', 'type']].apply(lambda x: translate_regular(*x), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f2242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_postfix = \"preposing+sprinkles\"\n",
    "train_df.to_csv(f'./cogs_{dataset_postfix}/train.tsv', sep='\\t', index=False, header=False)\n",
    "dev_df.to_csv(f'./cogs_{dataset_postfix}/dev.tsv', sep='\\t', index=False, header=False)\n",
    "test_df.to_csv(f'./cogs_{dataset_postfix}/test.tsv', sep='\\t', index=False, header=False)\n",
    "gen_df.to_csv(f'./cogs_{dataset_postfix}/gen.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d755a5d7",
   "metadata": {},
   "source": [
    "### ReCOGS (Number of resampling iterations = 5)\n",
    "It seems like the performance gain from increasing the number of resampling iterations dimish quickly after getting the number above 10. We are trying 5 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2541f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_digit_pool = set([])\n",
    "# loading target vocab to random sample our variable names\n",
    "for k, v in load_vocab(\"./cogs/tgt_vocab.txt\").items():\n",
    "    if k.isnumeric():\n",
    "        existing_digit_pool.add(k)\n",
    "existing_digit_pool = list(existing_digit_pool)\n",
    "\n",
    "def translate(text, phi):\n",
    "    \n",
    "    if len(phi.split()) == 1:\n",
    "        return text, phi\n",
    "    elif \"LAMBDA\" in phi:\n",
    "        phi_split = phi.split()\n",
    "        if len(phi_split) == 7:\n",
    "            return text, phi\n",
    "        v_pos = []\n",
    "        idx = 0\n",
    "        for t in phi_split:\n",
    "            if t == text:\n",
    "                v_pos += [idx]\n",
    "            idx += 1\n",
    "        for p in v_pos:\n",
    "            phi_split[p] = phi_split[p+2]\n",
    "            phi_split[p+2] = text\n",
    "        \n",
    "        return text, \" \".join(phi_split)\n",
    "    \n",
    "    # parse\n",
    "    text_split = text.split()\n",
    "    data = []    \n",
    "    conjs = re.split(r\"\\s*(?:AND|;)\\s*\", phi)\n",
    "    for conj in conjs: \n",
    "        if np_re.search(conj):\n",
    "            d = parse_np(conj)\n",
    "        elif pred_re.search(conj):\n",
    "            d = parse_pred(conj)\n",
    "        elif mod_re.search(conj):\n",
    "            d = parse_mod(conj)\n",
    "        else:\n",
    "            raise Exception(f\"Conjunct could not be parsed: {conj}\")\n",
    "        data.append(d)\n",
    "    \n",
    "    # collect\n",
    "    def_terms = []\n",
    "    role_terms = []\n",
    "    nmod_terms = []\n",
    "    for d in data:\n",
    "        if d['type'] == 'np':\n",
    "            if d['definiteness'] == '*':\n",
    "                def_terms += [f\"* {d['pred']} ( {d['entvar']} )\"]\n",
    "            else:\n",
    "                def_terms += [f\"{d['pred']} ( {d['entvar']} )\"]\n",
    "        if d['type'] == 'role':\n",
    "            role_terms += [f\"{d['role']} . {d['pred']} ( {d['eventvar']} , {d['entvar']} )\"]\n",
    "        elif d['type'] == 'mod':\n",
    "            nmod_terms += [f\"nmod . {d['pred']} ( {d['e1']} , {d['e2']} )\"]\n",
    "            \n",
    "    # sort def_terms\n",
    "    def_terms = [*set(def_terms)]\n",
    "    def_terms.sort(key = lambda x: int(x.split()[-2]))    \n",
    "    rest_terms = nmod_terms + role_terms\n",
    "    \n",
    "    # combine\n",
    "    def_terms = \" ; \".join(def_terms)\n",
    "    if def_terms == \"\":\n",
    "        terms = \" AND \".join(rest_terms)\n",
    "    elif \" AND \".join(rest_terms) == \"\":\n",
    "        terms = def_terms\n",
    "    else:\n",
    "        terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "    \n",
    "    # final step, remove biases\n",
    "    current_digit_pool = set([])\n",
    "    for t in terms.split():\n",
    "        if t.isnumeric():\n",
    "            current_digit_pool.add(t)\n",
    "    current_digit_pool = list(current_digit_pool)\n",
    "    random.shuffle(current_digit_pool)\n",
    "    sample_random_digit = random.sample(existing_digit_pool, k=len(current_digit_pool))\n",
    "    digit_mapping = dict(zip(current_digit_pool, sample_random_digit))\n",
    "    \n",
    "    new_terms = []\n",
    "    for t in terms.split():\n",
    "        if t == \"_\" or t == \"x\":\n",
    "            continue\n",
    "        if t.isnumeric():\n",
    "            new_terms += [digit_mapping[t]]\n",
    "        else:\n",
    "            new_terms += [t]\n",
    "\n",
    "    terms = \" \".join(new_terms)\n",
    "    return text, terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e83a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_n = 5\n",
    "append_k = 3072\n",
    "\n",
    "train_dfs = []\n",
    "for i in range(sampled_n):\n",
    "    train_df_i = pd.read_csv(\"./cogs/train.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "    train_df_i[['sentence', 'LF']] = train_df_i[['sentence', 'LF']].apply(lambda x: translate(*x), axis=1, result_type='expand')\n",
    "    train_dfs += [train_df_i]\n",
    "    \n",
    "dev_df = pd.read_csv(\"./cogs/dev.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "test_df = pd.read_csv(\"./cogs/test.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "gen_df = pd.read_csv(\"./cogs/gen.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "dev_df[['sentence', 'LF']] = dev_df[['sentence', 'LF']].apply(lambda x: translate(*x), axis=1, result_type='expand')\n",
    "test_df[['sentence', 'LF']] = test_df[['sentence', 'LF']].apply(lambda x: translate(*x), axis=1, result_type='expand')\n",
    "gen_df[['sentence', 'LF']] = gen_df[['sentence', 'LF']].apply(lambda x: translate(*x), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab36386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex(LFs, existing_digit_pool):\n",
    "    curr_digit = set([])\n",
    "    for i in range(len(LFs)):\n",
    "        for item in LFs[i].split():\n",
    "            if item.isnumeric():\n",
    "                curr_digit.add((i, int(item)))\n",
    "    sampled_digits = random.sample(existing_digit_pool, k=len(curr_digit))\n",
    "    digit_map = {}\n",
    "    idx = 0\n",
    "    for d in list(curr_digit):\n",
    "        digit_map[d] = sampled_digits[idx]\n",
    "        idx += 1\n",
    "    \n",
    "    reindex_LFs = []\n",
    "    for i in range(len(LFs)):\n",
    "        new_LFs = []\n",
    "        for item in LFs[i].split():\n",
    "            if item.isnumeric():\n",
    "                new_LFs += [digit_map[(i, int(item))]]\n",
    "            else:\n",
    "                new_LFs += [item]\n",
    "        reindex_LFs += [\" \".join(new_LFs)]\n",
    "        \n",
    "    new_LF_prefix = []\n",
    "    new_LF_body_nmod = []\n",
    "    new_LF_body_verb = []\n",
    "        \n",
    "    for i in range(len(reindex_LFs)):\n",
    "        new_LF_prefix.extend(reindex_LFs[i].split(\" ; \")[:-1])\n",
    "        for term in reindex_LFs[i].split(\" ; \")[-1].split(\" AND \"):\n",
    "            if \"nmod\" in term:\n",
    "                new_LF_body_nmod += [term]\n",
    "            else:\n",
    "                new_LF_body_verb += [term]\n",
    "                \n",
    "    new_LF_body = new_LF_body_nmod + new_LF_body_verb\n",
    "        \n",
    "    return \" ; \".join(new_LF_prefix) + \" ; \" + \" AND \".join(new_LF_body)\n",
    "\n",
    "start_indexes = [i*6 for i in range(append_k)]\n",
    "append_data = []\n",
    "\n",
    "for i in range(sampled_n):\n",
    "    train_df_sorted = train_dfs[i].sort_values(by=\"sentence\", key=lambda x: x.str.len())\n",
    "    for start_index in start_indexes:\n",
    "        append_data += [\n",
    "            [train_df_sorted.iloc[-1-start_index].sentence[:-1]+\", \"+\\\n",
    "            train_df_sorted.iloc[-2-start_index].sentence[0].lower()+\\\n",
    "            train_df_sorted.iloc[-2-start_index].sentence[1:-1]+\", \"+\\\n",
    "            train_df_sorted.iloc[-3-start_index].sentence[0].lower()+\\\n",
    "            train_df_sorted.iloc[-3-start_index].sentence[1:],\n",
    "            reindex(\n",
    "                [\n",
    "                    train_df_sorted.iloc[-1-start_index].LF,\n",
    "                    train_df_sorted.iloc[-2-start_index].LF,\n",
    "                    train_df_sorted.iloc[-3-start_index].LF\n",
    "                ], existing_digit_pool\n",
    "            ),\n",
    "            'length_ood']\n",
    "        ]\n",
    "append_data = pd.DataFrame(append_data, columns =['sentence', 'LF', 'type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471bc3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat(train_dfs)\n",
    "train_df = pd.concat([train_df, append_df])\n",
    "\n",
    "dataset_postfix = \"recogs\"\n",
    "train_df.to_csv(f'./{dataset_postfix}/train.tsv', sep='\\t', index=False, header=False)\n",
    "dev_df.to_csv(f'./{dataset_postfix}/dev.tsv', sep='\\t', index=False, header=False)\n",
    "test_df.to_csv(f'./{dataset_postfix}/test.tsv', sep='\\t', index=False, header=False)\n",
    "gen_df.to_csv(f'./{dataset_postfix}/gen.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc67be3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
