{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2a44d37",
   "metadata": {},
   "source": [
    "## Our reformatting functions\n",
    "- Redundant token removal\n",
    "- Example concatenation for longer training sequences\n",
    "- Preposing\n",
    "- Preposing + Interjection\n",
    "- ReCOGS\n",
    "- Variable-free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb833a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, random, copy\n",
    "\n",
    "import pandas as pd\n",
    "import re, random, copy\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import collections\n",
    "import unicodedata\n",
    "import torch\n",
    "import six\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "np_re = re.compile(r\"\"\"\n",
    "    ^\n",
    "    \\s*(\\*)?\n",
    "    \\s*(\\w+?)\\s*\n",
    "    \\(\n",
    "    \\s*(.+?)\\s*\n",
    "    \\)\n",
    "    \\s*$\"\"\", re.VERBOSE)\n",
    "\n",
    "pred_re = re.compile(r\"\"\"\n",
    "    ^\n",
    "    \\s*(\\w+?)\\s*\n",
    "    \\.\n",
    "    \\s*(\\w+?)\\s*\n",
    "    \\(\n",
    "    \\s*(.+?)\\s*\n",
    "    ,\n",
    "    \\s*(.+?)\\s*\n",
    "    \\)\n",
    "    \\s*$\"\"\", re.VERBOSE)\n",
    "\n",
    "mod_re = re.compile(r\"\"\"\n",
    "    ^\n",
    "    \\s*(\\w+?)\\s*\n",
    "    \\.\n",
    "    \\s*(\\w+?)\\s*\n",
    "    \\.\n",
    "    \\s*(\\w+?)\\s*\n",
    "    \\(\n",
    "    \\s*(.+?)\\s*\n",
    "    ,\n",
    "    \\s*(.+?)\\s*\n",
    "    \\)\n",
    "    \\s*$\"\"\", re.VERBOSE)\n",
    "\n",
    "\n",
    "def parse_np(phi):   \n",
    "    the, pred, var = np_re.search(phi).groups()\n",
    "    indef = '' if the is None else '*'\n",
    "    return {'type': 'np', 'definiteness': indef, 'pred': pred, 'entvar': var}\n",
    "\n",
    "def parse_pred(phi):\n",
    "    pred, role, eventvar, entvar = pred_re.search(phi).groups()\n",
    "    return {'type': 'role', 'role': role, 'pred': pred, 'entvar': entvar, 'eventvar': eventvar}\n",
    "\n",
    "def parse_mod(phi):\n",
    "    nppred, rel, pred, e1, e2 = mod_re.search(phi).groups()\n",
    "    # Keeping `rel` even though it is always 'nmod'\n",
    "    return {'type': 'mod', 'rel': rel, 'pred': pred, 'nppred': nppred, 'e1': e1, 'e2': e2}\n",
    "\n",
    "def translate_entity_simplied(entvar, data):\n",
    "    ent = [e for e in data if e['type'] == 'np' and e.get(\"entvar\") == entvar]\n",
    "    if not ent:\n",
    "        return entvar, entvar\n",
    "    else:\n",
    "        ent = ent[0]\n",
    "        return f\"{ent['definiteness']} {ent['entvar']} ( {ent['pred']} )\", f\"{ent['definiteness']} {ent['pred']}\"\n",
    "\n",
    "def translate_entity(entvar, data):\n",
    "    ent = [e for e in data if e['type'] == 'np' and e.get(\"entvar\") == entvar]\n",
    "    if not ent:\n",
    "        return entvar\n",
    "    else:\n",
    "        ent = ent[0]\n",
    "        return f\"{ent['definiteness']} {ent['entvar']} ( {ent['pred']} )\"\n",
    "    \n",
    "def convert_to_unicode(text):\n",
    "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "    if six.PY3:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, bytes):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    elif six.PY2:\n",
    "        if isinstance(text, str):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        elif isinstance(text, unicode):\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    else:\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")\n",
    "\n",
    "def convert_tokens_to_ids(vocab, tokens):\n",
    "    \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n",
    "    ids = []\n",
    "    for token in tokens:\n",
    "        if token not in vocab.keys():\n",
    "            ids.append(vocab['[UNK]'])\n",
    "        else:\n",
    "            ids.append(vocab[token])\n",
    "    return ids\n",
    "        \n",
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = collections.OrderedDict()\n",
    "    index = 0\n",
    "    with open(vocab_file, \"r\") as reader:\n",
    "        while True:\n",
    "            token = convert_to_unicode(reader.readline())\n",
    "            if not token:\n",
    "                break\n",
    "            token = token.strip()\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c03764",
   "metadata": {},
   "source": [
    "### Redundant Token Removal\n",
    "TODO: currently, we just have to change the removing set of tokens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dda3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_removal(text, phi): \n",
    "    global all_roles\n",
    "    removing_set = {'x', '_', '(', ')', ','}\n",
    "    # Parsing:\n",
    "    terms = []\n",
    "    for t in phi.split():\n",
    "        if t not in removing_set:\n",
    "            terms += [t]\n",
    "    ret = \" \".join(terms).strip()\n",
    "    return ret\n",
    "train_df = pd.read_csv(\"./cogs/train.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "dev_df = pd.read_csv(\"./cogs/dev.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "test_df = pd.read_csv(\"./cogs/test.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "gen_df = pd.read_csv(\"./cogs/gen.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "\n",
    "train_df['LF'] = train_df[['sentence', 'LF']].apply(lambda x: token_removal(*x), axis=1,)\n",
    "dev_df['LF'] = dev_df[['sentence', 'LF']].apply(lambda x: token_removal(*x), axis=1,)\n",
    "test_df['LF'] = test_df[['sentence', 'LF']].apply(lambda x: token_removal(*x), axis=1,)\n",
    "gen_df['LF'] = gen_df[['sentence', 'LF']].apply(lambda x: token_removal(*x), axis=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746c3bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_postfix = \"remove_x_(,)\"\n",
    "train_df.to_csv(f'./cogs_token_removal/train_{dataset_postfix}.tsv', sep='\\t', index=False, header=False)\n",
    "dev_df.to_csv(f'./cogs_token_removal/dev_{dataset_postfix}.tsv', sep='\\t', index=False, header=False)\n",
    "test_df.to_csv(f'./cogs_token_removal/test_{dataset_postfix}.tsv', sep='\\t', index=False, header=False)\n",
    "gen_df.to_csv(f'./cogs_token_removal/gen_{dataset_postfix}.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c1d099",
   "metadata": {},
   "source": [
    "### Example Concatenations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cabc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex(LFs, initial_indexes):\n",
    "    new_LF_prefix = []\n",
    "    new_LF_body = []\n",
    "    for i in range(len(LFs)):\n",
    "        if initial_indexes[i] != 0:\n",
    "            new_lf = []\n",
    "            for item in LFs[i].split():\n",
    "                if item.isnumeric():\n",
    "                    new_i = int(item) + initial_indexes[i]\n",
    "                    new_lf += [str(new_i)]\n",
    "                else:\n",
    "                    new_lf += [item]\n",
    "            new_lf = \" \".join(new_lf)\n",
    "        else:\n",
    "            new_lf = LFs[i]\n",
    "        \n",
    "        for item in new_lf.split(\" ; \"):\n",
    "            if \"*\" in item:\n",
    "                new_LF_prefix += [item]\n",
    "            else:\n",
    "                new_LF_body += [item]\n",
    "        new_LF_body += [\"AND\"]\n",
    "    new_LF_body = new_LF_body[:-1]\n",
    "    return \" ; \".join(new_LF_prefix) + \" ; \" + \" \".join(new_LF_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7785a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_ks = [256, 512, 1024, 2048, 3072]\n",
    "for append_k in append_ks:\n",
    "    train_df = pd.read_csv(\"./cogs/train.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "    train_df_org = train_df.copy()\n",
    "    train_df = train_df[train_df[\"type\"] != \"primitive\"]\n",
    "    dev_df = pd.read_csv(\"./cogs/dev.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "    test_df = pd.read_csv(\"./cogs/test.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "    gen_df = pd.read_csv(\"./cogs/gen.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "    dataset_postfix = f\"k_{append_k}\"\n",
    "    append_data = []\n",
    "    start_indexes = [i*6 for i in range(append_k)]\n",
    "    sorted_train_df = train_df.sort_values(by=\"sentence\", key=lambda x: x.str.len())\n",
    "    for start_index in start_indexes:\n",
    "        append_data += [\n",
    "            [sorted_train_df.iloc[-1-start_index].sentence[:-1]+\\\n",
    "            sorted_train_df.iloc[-2-start_index].sentence[0].lower()+\\\n",
    "            sorted_train_df.iloc[-2-start_index].sentence[1:-1]+\\\n",
    "            sorted_train_df.iloc[-3-start_index].sentence[0].lower()+\\\n",
    "            sorted_train_df.iloc[-3-start_index].sentence[1:],\n",
    "            reindex(\n",
    "                [\n",
    "                    sorted_train_df.iloc[-1-start_index].LF,\n",
    "                    sorted_train_df.iloc[-2-start_index].LF,\n",
    "                    sorted_train_df.iloc[-3-start_index].LF\n",
    "                ],\n",
    "                [\n",
    "                    0,\n",
    "                    len(sorted_train_df.iloc[-1-start_index].sentence[:-1].strip().split()),\n",
    "                    len(sorted_train_df.iloc[-1-start_index].sentence[:-1].strip().split())+\n",
    "                    len(sorted_train_df.iloc[-2-start_index].sentence[:-1].strip().split())\n",
    "                ]\n",
    "            ),\n",
    "            'concat']\n",
    "        ]\n",
    "    append_df = pd.DataFrame(append_data, columns =['sentence', 'LF', 'type'])\n",
    "    train_df = pd.concat([train_df_org, append_df])\n",
    "    train_df.to_csv(f'./cogs_concat/train_{dataset_postfix}.tsv', sep='\\t', index=False, header=False)\n",
    "    dev_df.to_csv(f'./cogs_concat/dev_{dataset_postfix}.tsv', sep='\\t', index=False, header=False)\n",
    "    test_df.to_csv(f'./cogs_concat/test_{dataset_postfix}.tsv', sep='\\t', index=False, header=False)\n",
    "    gen_df.to_csv(f'./cogs_concat/gen_{dataset_postfix}.tsv', sep='\\t', index=False, header=False)\n",
    "    \n",
    "    max_s = max(train_df['sentence'].str.split().apply(len))\n",
    "    max_lf = max(train_df['LF'].str.split().apply(len))\n",
    "    print(max_s, max_lf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed5eb6f",
   "metadata": {},
   "source": [
    "### Preposing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845703e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_regular(text, phi, _type):\n",
    "    \n",
    "    if len(phi.split()) == 1:\n",
    "        return text, phi, _type\n",
    "    elif \"LAMBDA\" in phi:\n",
    "        return text, phi, _type\n",
    "    \n",
    "    # parse\n",
    "    text_split = text.split()\n",
    "    data = []    \n",
    "    conjs = re.split(r\"\\s*(?:AND|;)\\s*\", phi)\n",
    "    for conj in conjs: \n",
    "        if np_re.search(conj):\n",
    "            d = parse_np(conj)\n",
    "        elif pred_re.search(conj):\n",
    "            d = parse_pred(conj)\n",
    "        elif mod_re.search(conj):\n",
    "            d = parse_mod(conj)\n",
    "        else:\n",
    "            raise Exception(f\"Conjunct could not be parsed: {conj}\")\n",
    "        data.append(d)\n",
    "    \n",
    "    # collect\n",
    "    def_terms = []\n",
    "    role_terms = []\n",
    "    nmod_terms = []\n",
    "    for d in data:\n",
    "        if d['type'] == 'np':\n",
    "            if d['definiteness'] == '*':\n",
    "                def_terms += [f\"* {d['pred']} ( {d['entvar']} )\"]\n",
    "            else:\n",
    "                role_terms += [f\"{d['pred']} ( {d['entvar']} )\"]\n",
    "        if d['type'] == 'role':\n",
    "            role_terms += [f\"{d['pred']} . {d['role']} ( {d['eventvar']} , {d['entvar']} )\"]\n",
    "        elif d['type'] == 'mod':\n",
    "            assert \"x _\" in d['e1']\n",
    "            role_terms += [f\"{d['nppred']} . nmod . {d['pred']} ( {d['e1']} , {d['e2']} )\"]\n",
    "            \n",
    "    # sort def_terms\n",
    "    def_terms = [*set(def_terms)]\n",
    "    def_terms.sort(key = lambda x: int(x.split()[-2]))    \n",
    "    rest_terms = role_terms\n",
    "    \n",
    "    # combine\n",
    "    def_terms = \" ; \".join(def_terms)\n",
    "    if def_terms == \"\":\n",
    "        terms = \" AND \".join(rest_terms)\n",
    "    elif \" AND \".join(rest_terms) == \"\":\n",
    "        terms = def_terms\n",
    "    else:\n",
    "        terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "\n",
    "    return text, terms, _type\n",
    "        \n",
    "def translate(text, phi, _type):\n",
    "    \n",
    "    if len(phi.split()) == 1:\n",
    "        return text, phi, _type\n",
    "    elif \"LAMBDA\" in phi:\n",
    "        return text, phi, _type\n",
    "    \n",
    "    # parse\n",
    "    text_split = text.split()\n",
    "    data = []    \n",
    "    conjs = re.split(r\"\\s*(?:AND|;)\\s*\", phi)\n",
    "    for conj in conjs: \n",
    "        if np_re.search(conj):\n",
    "            d = parse_np(conj)\n",
    "        elif pred_re.search(conj):\n",
    "            d = parse_pred(conj)\n",
    "        elif mod_re.search(conj):\n",
    "            d = parse_mod(conj)\n",
    "        else:\n",
    "            raise Exception(f\"Conjunct could not be parsed: {conj}\")\n",
    "        data.append(d)\n",
    "    \n",
    "    # collect\n",
    "    def_terms = []\n",
    "    role_terms = []\n",
    "    nmod_terms = []\n",
    "    for d in data:\n",
    "        if d['type'] == 'np':\n",
    "            if d['definiteness'] == '*':\n",
    "                def_terms += [f\"* {d['pred']} ( {d['entvar']} )\"]\n",
    "            else:\n",
    "                role_terms += [f\"{d['pred']} ( {d['entvar']} )\"]\n",
    "        if d['type'] == 'role':\n",
    "            role_terms += [f\"{d['pred']} . {d['role']} ( {d['eventvar']} , {d['entvar']} )\"]\n",
    "        elif d['type'] == 'mod':\n",
    "            assert \"x _\" in d['e1']\n",
    "            role_terms += [f\"{d['nppred']} . nmod . {d['pred']} ( {d['e1']} , {d['e2']} )\"]\n",
    "            \n",
    "    # sort def_terms\n",
    "    def_terms = [*set(def_terms)]\n",
    "    def_terms.sort(key = lambda x: int(x.split()[-2]))    \n",
    "    rest_terms = role_terms\n",
    "    \n",
    "    # combine\n",
    "    def_terms = \" ; \".join(def_terms)\n",
    "    if def_terms == \"\":\n",
    "        terms = \" AND \".join(rest_terms)\n",
    "    elif \" AND \".join(rest_terms) == \"\":\n",
    "        terms = def_terms\n",
    "    else:\n",
    "        terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "    \n",
    "    if \"nmod\" not in terms.split():\n",
    "        return text, terms, _type\n",
    "    \n",
    "    if random.random() >= proposing_prob:\n",
    "        return text, terms, _type\n",
    "    \n",
    "    if terms.split().count(\"nmod\") == 2:\n",
    "        upper_bound = 6\n",
    "        return_type = \"preposing_2\"\n",
    "    elif terms.split().count(\"nmod\") == 1:\n",
    "        upper_bound = 3\n",
    "        return_type = \"preposing_1\"\n",
    "    else:\n",
    "        assert False\n",
    "        \n",
    "    nmod = terms.split()[terms.split().index(\"nmod\")+2]\n",
    "    pre_phrase = text.split()[text.split().index(nmod)-2 : text.split().index(nmod)+upper_bound]\n",
    "    pre_phrase[0] = pre_phrase[0].capitalize()\n",
    "    pre_text = text.split()[:text.split().index(nmod)-2]\n",
    "    if pre_text[0] in [\"The\", \"A\"]:\n",
    "        pre_text[0] = pre_text[0].lower()\n",
    "    post_text = text.split()[text.split().index(nmod)+upper_bound:]\n",
    "    pre_text = pre_phrase + pre_text + post_text\n",
    "    pre_text = \" \".join(pre_text)\n",
    "\n",
    "    index_map = {}\n",
    "    idx = 0\n",
    "    for i in range(text.split().index(nmod)-2, text.split().index(nmod)+upper_bound):\n",
    "        index_map[f\"{i}\"] = f\"{idx}\"\n",
    "        idx += 1\n",
    "    for i in range(text.split().index(nmod)-2):\n",
    "        ii = 2+upper_bound+i\n",
    "        index_map[f\"{i}\"] = f\"{ii}\"\n",
    "        idx += 1\n",
    "    for i in range(text.split().index(nmod)+upper_bound, len(text.split())):\n",
    "        ii = idx+(i-(text.split().index(nmod)+upper_bound))\n",
    "        index_map[f\"{i}\"] = f\"{ii}\"  \n",
    "            \n",
    "    # now handle LF\n",
    "    pre_terms = []\n",
    "    for t in terms.split():\n",
    "        if t.isnumeric():\n",
    "            pre_terms += [str(index_map[str(int(t))])]\n",
    "        else:\n",
    "            pre_terms += [t]\n",
    "    pre_terms = \" \".join(pre_terms)\n",
    "\n",
    "    pre_terms_def = pre_terms.split(\" ; \")[:-1]\n",
    "    pre_terms_def.sort(key = lambda x: int(x.split()[-2]))  \n",
    "    pre_terms_role = pre_terms.split(\" ; \")[-1].split(\" AND \")\n",
    "    pre_terms_role.sort(key = lambda x: (int(x.split()[-2]) if x.split()[-5] == \"(\" else int(x.split()[-6]) if x.split()[-2].isnumeric() else int(x.split()[-4]), -1 if x.split()[-5] == \"(\" else int(x.split()[-2]) if x.split()[-2].isnumeric() else pre_text.split().index(x.split()[-2])))  \n",
    "    pre_terms_role = \" AND \".join(pre_terms_role)\n",
    "    pre_terms = \" ; \".join(pre_terms_def + [pre_terms_role])\n",
    "\n",
    "    return pre_text, pre_terms, return_type\n",
    "\n",
    "train_df = pd.read_csv(\"./cogs/train.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "dev_df = pd.read_csv(\"./cogs/dev.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "test_df = pd.read_csv(\"./cogs/test.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "gen_df = pd.read_csv(\"./cogs/gen.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "\n",
    "proposing_prob = 0.05\n",
    "train_df[['sentence', 'LF', 'type']] = train_df[['sentence', 'LF', 'type']].apply(lambda x: translate(*x), axis=1, result_type='expand')\n",
    "dev_df[['sentence', 'LF', 'type']] = dev_df[['sentence', 'LF', 'type']].apply(lambda x: translate_regular(*x), axis=1, result_type='expand')\n",
    "test_df[['sentence', 'LF', 'type']] = test_df[['sentence', 'LF', 'type']].apply(lambda x: translate_regular(*x), axis=1, result_type='expand')\n",
    "gen_df[['sentence', 'LF', 'type']] = gen_df[['sentence', 'LF', 'type']].apply(lambda x: translate_regular(*x), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d26b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_postfix = \"preposing\"\n",
    "train_df.to_csv(f'./cogs_{dataset_postfix}/train.tsv', sep='\\t', index=False, header=False)\n",
    "dev_df.to_csv(f'./cogs_{dataset_postfix}/dev.tsv', sep='\\t', index=False, header=False)\n",
    "test_df.to_csv(f'./cogs_{dataset_postfix}/test.tsv', sep='\\t', index=False, header=False)\n",
    "gen_df.to_csv(f'./cogs_{dataset_postfix}/gen.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96411af9",
   "metadata": {},
   "source": [
    "### Preposing + Sprinkles (Interjection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f5c65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_regular(text, phi, _type):\n",
    "    \n",
    "    if len(phi.split()) == 1:\n",
    "        return text, phi, _type\n",
    "    elif \"LAMBDA\" in phi:\n",
    "        return text, phi, _type\n",
    "    \n",
    "    # parse\n",
    "    text_split = text.split()\n",
    "    data = []    \n",
    "    conjs = re.split(r\"\\s*(?:AND|;)\\s*\", phi)\n",
    "    for conj in conjs: \n",
    "        if np_re.search(conj):\n",
    "            d = parse_np(conj)\n",
    "        elif pred_re.search(conj):\n",
    "            d = parse_pred(conj)\n",
    "        elif mod_re.search(conj):\n",
    "            d = parse_mod(conj)\n",
    "        else:\n",
    "            raise Exception(f\"Conjunct could not be parsed: {conj}\")\n",
    "        data.append(d)\n",
    "    \n",
    "    # collect\n",
    "    def_terms = []\n",
    "    role_terms = []\n",
    "    nmod_terms = []\n",
    "    for d in data:\n",
    "        if d['type'] == 'np':\n",
    "            if d['definiteness'] == '*':\n",
    "                def_terms += [f\"* {d['pred']} ( {d['entvar']} )\"]\n",
    "            else:\n",
    "                role_terms += [f\"{d['pred']} ( {d['entvar']} )\"]\n",
    "        if d['type'] == 'role':\n",
    "            role_terms += [f\"{d['pred']} . {d['role']} ( {d['eventvar']} , {d['entvar']} )\"]\n",
    "        elif d['type'] == 'mod':\n",
    "            assert \"x _\" in d['e1']\n",
    "            role_terms += [f\"{d['nppred']} . nmod . {d['pred']} ( {d['e1']} , {d['e2']} )\"]\n",
    "            \n",
    "    # sort def_terms\n",
    "    def_terms = [*set(def_terms)]\n",
    "    def_terms.sort(key = lambda x: int(x.split()[-2]))    \n",
    "    rest_terms = role_terms\n",
    "    \n",
    "    # combine\n",
    "    def_terms = \" ; \".join(def_terms)\n",
    "    if def_terms == \"\":\n",
    "        terms = \" AND \".join(rest_terms)\n",
    "    elif \" AND \".join(rest_terms) == \"\":\n",
    "        terms = def_terms\n",
    "    else:\n",
    "        terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "\n",
    "    return text, terms, _type\n",
    "        \n",
    "def translate(text, phi, _type):\n",
    "    \n",
    "    if len(phi.split()) == 1:\n",
    "        return text, phi, _type\n",
    "    elif \"LAMBDA\" in phi:\n",
    "        return text, phi, _type\n",
    "    \n",
    "    # parse\n",
    "    text_split = text.split()\n",
    "    data = []    \n",
    "    conjs = re.split(r\"\\s*(?:AND|;)\\s*\", phi)\n",
    "    for conj in conjs: \n",
    "        if np_re.search(conj):\n",
    "            d = parse_np(conj)\n",
    "        elif pred_re.search(conj):\n",
    "            d = parse_pred(conj)\n",
    "        elif mod_re.search(conj):\n",
    "            d = parse_mod(conj)\n",
    "        else:\n",
    "            raise Exception(f\"Conjunct could not be parsed: {conj}\")\n",
    "        data.append(d)\n",
    "    \n",
    "    # collect\n",
    "    def_terms = []\n",
    "    role_terms = []\n",
    "    nmod_terms = []\n",
    "    for d in data:\n",
    "        if d['type'] == 'np':\n",
    "            if d['definiteness'] == '*':\n",
    "                def_terms += [f\"* {d['pred']} ( {d['entvar']} )\"]\n",
    "            else:\n",
    "                role_terms += [f\"{d['pred']} ( {d['entvar']} )\"]\n",
    "        if d['type'] == 'role':\n",
    "            role_terms += [f\"{d['pred']} . {d['role']} ( {d['eventvar']} , {d['entvar']} )\"]\n",
    "        elif d['type'] == 'mod':\n",
    "            assert \"x _\" in d['e1']\n",
    "            role_terms += [f\"{d['nppred']} . nmod . {d['pred']} ( {d['e1']} , {d['e2']} )\"]\n",
    "            \n",
    "    # sort def_terms\n",
    "    def_terms = [*set(def_terms)]\n",
    "    def_terms.sort(key = lambda x: int(x.split()[-2]))    \n",
    "    rest_terms = role_terms\n",
    "    \n",
    "    # combine\n",
    "    def_terms = \" ; \".join(def_terms)\n",
    "    if def_terms == \"\":\n",
    "        terms = \" AND \".join(rest_terms)\n",
    "    elif \" AND \".join(rest_terms) == \"\":\n",
    "        terms = def_terms\n",
    "    else:\n",
    "        terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "    \n",
    "    if \"nmod\" not in terms.split():\n",
    "        return text, terms, _type\n",
    "    \n",
    "    if random.random() >= proposing_prob:\n",
    "        return text, terms, _type\n",
    "    \n",
    "    if terms.split().count(\"nmod\") == 2:\n",
    "        upper_bound = 6\n",
    "        return_type = \"preposing_2\"\n",
    "    elif terms.split().count(\"nmod\") == 1:\n",
    "        upper_bound = 3\n",
    "        return_type = \"preposing_1\"\n",
    "    else:\n",
    "        assert False\n",
    "        \n",
    "    nmod = terms.split()[terms.split().index(\"nmod\")+2]\n",
    "    pre_phrase = text.split()[text.split().index(nmod)-2 : text.split().index(nmod)+upper_bound]\n",
    "    pre_phrase[0] = pre_phrase[0].capitalize()\n",
    "    pre_text = text.split()[:text.split().index(nmod)-2]\n",
    "    if pre_text[0] in [\"The\", \"A\"]:\n",
    "        pre_text[0] = pre_text[0].lower()\n",
    "    post_text = text.split()[text.split().index(nmod)+upper_bound:]\n",
    "    pre_text = pre_phrase + pre_text + post_text\n",
    "    pre_text = \" \".join(pre_text)\n",
    "\n",
    "    index_map = {}\n",
    "    idx = 0\n",
    "    for i in range(text.split().index(nmod)-2, text.split().index(nmod)+upper_bound):\n",
    "        index_map[f\"{i}\"] = f\"{idx}\"\n",
    "        idx += 1\n",
    "    for i in range(text.split().index(nmod)-2):\n",
    "        ii = 2+upper_bound+i\n",
    "        index_map[f\"{i}\"] = f\"{ii}\"\n",
    "        idx += 1\n",
    "    for i in range(text.split().index(nmod)+upper_bound, len(text.split())):\n",
    "        ii = idx+(i-(text.split().index(nmod)+upper_bound))\n",
    "        index_map[f\"{i}\"] = f\"{ii}\"  \n",
    "            \n",
    "    # now handle LF\n",
    "    pre_terms = []\n",
    "    for t in terms.split():\n",
    "        if t.isnumeric():\n",
    "            pre_terms += [str(index_map[str(int(t))])]\n",
    "        else:\n",
    "            pre_terms += [t]\n",
    "    pre_terms = \" \".join(pre_terms)\n",
    "\n",
    "    pre_terms_def = pre_terms.split(\" ; \")[:-1]\n",
    "    pre_terms_def.sort(key = lambda x: int(x.split()[-2]))  \n",
    "    pre_terms_role = pre_terms.split(\" ; \")[-1].split(\" AND \")\n",
    "    pre_terms_role.sort(key = lambda x: (int(x.split()[-2]) if x.split()[-5] == \"(\" else int(x.split()[-6]) if x.split()[-2].isnumeric() else int(x.split()[-4]), -1 if x.split()[-5] == \"(\" else int(x.split()[-2]) if x.split()[-2].isnumeric() else pre_text.split().index(x.split()[-2])))  \n",
    "    pre_terms_role = \" AND \".join(pre_terms_role)\n",
    "    pre_terms = \" ; \".join(pre_terms_def + [pre_terms_role])\n",
    "\n",
    "    return pre_text, pre_terms, return_type\n",
    "\n",
    "def add_um(sentence):\n",
    "    words = sentence.split()\n",
    "    new_words = []\n",
    "    mapping = {}\n",
    "    offset = 0\n",
    "    for i, word in enumerate(words):\n",
    "        mapping[i] = len(new_words)\n",
    "        new_words.append(word)\n",
    "        if i > 0 and i < len(words) - 2 and random.random() > 0.5:\n",
    "            num_um = random.choice([1,2,3])\n",
    "            for j in range(num_um):\n",
    "                new_words.append(\"um\")\n",
    "    return \" \".join(new_words), mapping\n",
    "\n",
    "def sprinkle(text, phi, _type):\n",
    "    if \"preposition\" in _type:\n",
    "        return text, phi, _type\n",
    "    \n",
    "    if random.random() >= sprinkle_prob:\n",
    "        return text, phi, _type\n",
    "    \n",
    "    um_text, token_mapping = add_um(text)\n",
    "    um_phi = []\n",
    "    for t in phi.split():\n",
    "        if t.isnumeric():\n",
    "            um_phi += [str(token_mapping[int(t)])]\n",
    "        else:\n",
    "            um_phi += [t]\n",
    "    um_phi = \" \".join(um_phi)\n",
    "    \n",
    "    return um_text, um_phi, \"sprinkle\"\n",
    "            \n",
    "train_df = pd.read_csv(\"./cogs/train.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "dev_df = pd.read_csv(\"./cogs/dev.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "test_df = pd.read_csv(\"./cogs/test.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "gen_df = pd.read_csv(\"./cogs/gen.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "\n",
    "proposing_prob = 0.05\n",
    "sprinkle_prob = 0.05\n",
    "train_df[['sentence', 'LF', 'type']] = train_df[['sentence', 'LF', 'type']].apply(lambda x: translate(*x), axis=1, result_type='expand')\n",
    "train_df[['sentence', 'LF', 'type']] = train_df[['sentence', 'LF', 'type']].apply(lambda x: sprinkle(*x), axis=1, result_type='expand')\n",
    "dev_df[['sentence', 'LF', 'type']] = dev_df[['sentence', 'LF', 'type']].apply(lambda x: translate_regular(*x), axis=1, result_type='expand')\n",
    "test_df[['sentence', 'LF', 'type']] = test_df[['sentence', 'LF', 'type']].apply(lambda x: translate_regular(*x), axis=1, result_type='expand')\n",
    "gen_df[['sentence', 'LF', 'type']] = gen_df[['sentence', 'LF', 'type']].apply(lambda x: translate_regular(*x), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f2242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_postfix = \"preposing+sprinkles\"\n",
    "train_df.to_csv(f'./cogs_{dataset_postfix}/train.tsv', sep='\\t', index=False, header=False)\n",
    "dev_df.to_csv(f'./cogs_{dataset_postfix}/dev.tsv', sep='\\t', index=False, header=False)\n",
    "test_df.to_csv(f'./cogs_{dataset_postfix}/test.tsv', sep='\\t', index=False, header=False)\n",
    "gen_df.to_csv(f'./cogs_{dataset_postfix}/gen.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d755a5d7",
   "metadata": {},
   "source": [
    "### ReCOGS (Number of resampling iterations = 5)\n",
    "It seems like the performance gain from increasing the number of resampling iterations dimish quickly after getting the number above 10. We are trying 5 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2541f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_digit_pool = set([])\n",
    "# loading target vocab to random sample our variable names\n",
    "for k, v in load_vocab(\"./cogs/tgt_vocab.txt\").items():\n",
    "    if k.isnumeric():\n",
    "        existing_digit_pool.add(k)\n",
    "existing_digit_pool = list(existing_digit_pool)\n",
    "\n",
    "def translate(text, phi):\n",
    "    \n",
    "    if len(phi.split()) == 1:\n",
    "        return text, phi\n",
    "    elif \"LAMBDA\" in phi:\n",
    "        phi_split = phi.split()\n",
    "        if len(phi_split) == 7:\n",
    "            return text, phi\n",
    "        v_pos = []\n",
    "        idx = 0\n",
    "        for t in phi_split:\n",
    "            if t == text:\n",
    "                v_pos += [idx]\n",
    "            idx += 1\n",
    "        for p in v_pos:\n",
    "            phi_split[p] = phi_split[p+2]\n",
    "            phi_split[p+2] = text\n",
    "        \n",
    "        return text, \" \".join(phi_split)\n",
    "    \n",
    "    # parse\n",
    "    text_split = text.split()\n",
    "    data = []    \n",
    "    conjs = re.split(r\"\\s*(?:AND|;)\\s*\", phi)\n",
    "    for conj in conjs: \n",
    "        if np_re.search(conj):\n",
    "            d = parse_np(conj)\n",
    "        elif pred_re.search(conj):\n",
    "            d = parse_pred(conj)\n",
    "        elif mod_re.search(conj):\n",
    "            d = parse_mod(conj)\n",
    "        else:\n",
    "            raise Exception(f\"Conjunct could not be parsed: {conj}\")\n",
    "        data.append(d)\n",
    "    \n",
    "    # collect\n",
    "    def_terms = []\n",
    "    role_terms = []\n",
    "    nmod_terms = []\n",
    "    for d in data:\n",
    "        if d['type'] == 'np':\n",
    "            if d['definiteness'] == '*':\n",
    "                def_terms += [f\"* {d['pred']} ( {d['entvar']} )\"]\n",
    "            else:\n",
    "                def_terms += [f\"{d['pred']} ( {d['entvar']} )\"]\n",
    "        if d['type'] == 'role':\n",
    "            role_terms += [f\"{d['role']} . {d['pred']} ( {d['eventvar']} , {d['entvar']} )\"]\n",
    "        elif d['type'] == 'mod':\n",
    "            nmod_terms += [f\"nmod . {d['pred']} ( {d['e1']} , {d['e2']} )\"]\n",
    "            \n",
    "    # sort def_terms\n",
    "    def_terms = [*set(def_terms)]\n",
    "    def_terms.sort(key = lambda x: int(x.split()[-2]))    \n",
    "    rest_terms = nmod_terms + role_terms\n",
    "    \n",
    "    # combine\n",
    "    def_terms = \" ; \".join(def_terms)\n",
    "    if def_terms == \"\":\n",
    "        terms = \" AND \".join(rest_terms)\n",
    "    elif \" AND \".join(rest_terms) == \"\":\n",
    "        terms = def_terms\n",
    "    else:\n",
    "        terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "    \n",
    "    # final step, remove biases\n",
    "    current_digit_pool = set([])\n",
    "    for t in terms.split():\n",
    "        if t.isnumeric():\n",
    "            current_digit_pool.add(t)\n",
    "    current_digit_pool = list(current_digit_pool)\n",
    "    random.shuffle(current_digit_pool)\n",
    "    sample_random_digit = random.sample(existing_digit_pool, k=len(current_digit_pool))\n",
    "    digit_mapping = dict(zip(current_digit_pool, sample_random_digit))\n",
    "    \n",
    "    new_terms = []\n",
    "    for t in terms.split():\n",
    "        if t == \"_\" or t == \"x\":\n",
    "            continue\n",
    "        if t.isnumeric():\n",
    "            new_terms += [digit_mapping[t]]\n",
    "        else:\n",
    "            new_terms += [t]\n",
    "\n",
    "    terms = \" \".join(new_terms)\n",
    "    return text, terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0e83a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_n = 5\n",
    "append_k = 3072\n",
    "\n",
    "train_dfs = []\n",
    "for i in range(sampled_n):\n",
    "    train_df_i = pd.read_csv(\"./cogs/train.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "    train_df_i[['sentence', 'LF']] = train_df_i[['sentence', 'LF']].apply(lambda x: translate(*x), axis=1, result_type='expand')\n",
    "    train_dfs += [train_df_i]\n",
    "    \n",
    "dev_df = pd.read_csv(\"./cogs/dev.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "test_df = pd.read_csv(\"./cogs/test.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "gen_df = pd.read_csv(\"./cogs/gen.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "dev_df[['sentence', 'LF']] = dev_df[['sentence', 'LF']].apply(lambda x: translate(*x), axis=1, result_type='expand')\n",
    "test_df[['sentence', 'LF']] = test_df[['sentence', 'LF']].apply(lambda x: translate(*x), axis=1, result_type='expand')\n",
    "gen_df[['sentence', 'LF']] = gen_df[['sentence', 'LF']].apply(lambda x: translate(*x), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ab36386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex(LFs, existing_digit_pool):\n",
    "    curr_digit = set([])\n",
    "    for i in range(len(LFs)):\n",
    "        for item in LFs[i].split():\n",
    "            if item.isnumeric():\n",
    "                curr_digit.add((i, int(item)))\n",
    "    sampled_digits = random.sample(existing_digit_pool, k=len(curr_digit))\n",
    "    digit_map = {}\n",
    "    idx = 0\n",
    "    for d in list(curr_digit):\n",
    "        digit_map[d] = sampled_digits[idx]\n",
    "        idx += 1\n",
    "    \n",
    "    reindex_LFs = []\n",
    "    for i in range(len(LFs)):\n",
    "        new_LFs = []\n",
    "        for item in LFs[i].split():\n",
    "            if item.isnumeric():\n",
    "                new_LFs += [digit_map[(i, int(item))]]\n",
    "            else:\n",
    "                new_LFs += [item]\n",
    "        reindex_LFs += [\" \".join(new_LFs)]\n",
    "        \n",
    "    new_LF_prefix = []\n",
    "    new_LF_body_nmod = []\n",
    "    new_LF_body_verb = []\n",
    "        \n",
    "    for i in range(len(reindex_LFs)):\n",
    "        new_LF_prefix.extend(reindex_LFs[i].split(\" ; \")[:-1])\n",
    "        for term in reindex_LFs[i].split(\" ; \")[-1].split(\" AND \"):\n",
    "            if \"nmod\" in term:\n",
    "                new_LF_body_nmod += [term]\n",
    "            else:\n",
    "                new_LF_body_verb += [term]\n",
    "                \n",
    "    new_LF_body = new_LF_body_nmod + new_LF_body_verb\n",
    "        \n",
    "    return \" ; \".join(new_LF_prefix) + \" ; \" + \" AND \".join(new_LF_body)\n",
    "\n",
    "start_indexes = [i*6 for i in range(append_k)]\n",
    "append_data = []\n",
    "\n",
    "for i in range(sampled_n):\n",
    "    train_df_sorted = train_dfs[i].sort_values(by=\"sentence\", key=lambda x: x.str.len())\n",
    "    for start_index in start_indexes:\n",
    "        append_data += [\n",
    "            [train_df_sorted.iloc[-1-start_index].sentence[:-1]+\", \"+\\\n",
    "            train_df_sorted.iloc[-2-start_index].sentence[0].lower()+\\\n",
    "            train_df_sorted.iloc[-2-start_index].sentence[1:-1]+\", \"+\\\n",
    "            train_df_sorted.iloc[-3-start_index].sentence[0].lower()+\\\n",
    "            train_df_sorted.iloc[-3-start_index].sentence[1:],\n",
    "            reindex(\n",
    "                [\n",
    "                    train_df_sorted.iloc[-1-start_index].LF,\n",
    "                    train_df_sorted.iloc[-2-start_index].LF,\n",
    "                    train_df_sorted.iloc[-3-start_index].LF\n",
    "                ], existing_digit_pool\n",
    "            ),\n",
    "            'length_ood']\n",
    "        ]\n",
    "append_data = pd.DataFrame(append_data, columns =['sentence', 'LF', 'type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "471bc3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat(train_dfs)\n",
    "train_df = pd.concat([train_df, append_df])\n",
    "\n",
    "dataset_postfix = \"recogs\"\n",
    "train_df.to_csv(f'./{dataset_postfix}/train.tsv', sep='\\t', index=False, header=False)\n",
    "dev_df.to_csv(f'./{dataset_postfix}/dev.tsv', sep='\\t', index=False, header=False)\n",
    "test_df.to_csv(f'./{dataset_postfix}/test.tsv', sep='\\t', index=False, header=False)\n",
    "gen_df.to_csv(f'./{dataset_postfix}/gen.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3bc584",
   "metadata": {},
   "source": [
    "### Variable-free format\n",
    "This is from Qiu et. al., 2022 paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e28d543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cogs_lf_to_funcall(lf):\n",
    "    \"\"\"Converts the given COGS logical form into the variable-free form.\n",
    "    - Nouns (entities and unaries) become values:\n",
    "      Jack --> Jack\n",
    "      cat ( x _ 1 ) --> cat\n",
    "      * cat ( x _ 1 ) --> * cat\n",
    "    - Verbs become functions, and their roles become argument names:\n",
    "      eat . agent ( x _ 2 , Jack ) --> eat ( agent = Jack )\n",
    "    - The variables representing nouns resolve to their values:\n",
    "      cat ( x _ 1 ) AND eat . agent ( x _ 2 , x _ 1 ) --> eat ( agent = cat )\n",
    "    This converter constructs a graph where variables are nodes and binaries\n",
    "    are edges. After identifying the root, it then performs depth-first traversal\n",
    "    to construct the output.\n",
    "    Args:\n",
    "    lf: Logical form string.\n",
    "    Returns:\n",
    "    The converted logical form.\n",
    "    \"\"\"\n",
    "    if \"LAMBDA\" in lf or \"(\" not in lf:\n",
    "        return lf\n",
    "\n",
    "    # Parse the terms in the logical form\n",
    "    # Example: toss . agent ( x _ 1 , John ) --> [toss, agent], [x _ 1, John]\n",
    "    terms = []\n",
    "    for raw_term in re.split(\" ; | AND \", lf):\n",
    "        match = re.match(r\"(.*) \\( (.*) \\)\", raw_term)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Malformed term: {raw_term}\")\n",
    "        labels = match.group(1).split(\" . \")\n",
    "        args = match.group(2).split(\" , \")\n",
    "        if len(args) not in (1, 2):\n",
    "            raise ValueError(f\"Invalid number of args: {args}\")\n",
    "        terms.append((labels, args))\n",
    "\n",
    "    # `nodes` maps variables to node name (e.g., \"x _ 3\" -> \"* cat\").\n",
    "    nodes = {}\n",
    "    for labels, args in terms:\n",
    "        if args[0] in nodes:\n",
    "            # The variable has already been seen; check for conflicts.\n",
    "            if nodes[args[0]] not in (labels[0], \"* \" + labels[0]):\n",
    "                raise ValueError(\n",
    "                    f\"Conflicting node name: {nodes[args[0]]} vs. {labels[0]}\")\n",
    "        else:\n",
    "            nodes[args[0]] = labels[0]\n",
    "\n",
    "    # `children` maps variables to a list of (edge name, target node).\n",
    "    children = {}\n",
    "    # Potential root nodes; any node being a child will be removed.\n",
    "    root_candidates = list(nodes)\n",
    "    for labels, args in terms:\n",
    "        if len(args) == 2:\n",
    "            if args[0] not in children:\n",
    "                children[args[0]] = []\n",
    "            children[args[0]].append((\" . \".join(labels[1:]), args[1]))\n",
    "            if args[1] in root_candidates:\n",
    "                root_candidates.remove(args[1])\n",
    "    if len(root_candidates) != 1:\n",
    "        raise ValueError(f\"Multiple roots: {root_candidates}\")\n",
    "    root = root_candidates[0]\n",
    "\n",
    "    # Depth-first traverse the graph to construct the funcall\n",
    "    def dfs(node):\n",
    "        if node not in nodes:\n",
    "            # Named entity such as \"John\"\n",
    "            if node.startswith(\"x _\"):\n",
    "                raise ValueError(f\"Unbound variable {node}\")\n",
    "            if node in children:\n",
    "                raise ValueError(f\"Named entity {node} has children {children[node]}\")\n",
    "            return [node]\n",
    "        else:\n",
    "            # A noun like \"cat\" or a verb like \"jump\"\n",
    "            if node not in children:\n",
    "                return [nodes[node]]\n",
    "            funcall_args = []\n",
    "            for edge_label, edge_target in children[node]:\n",
    "                funcall_args.append([edge_label, \"=\"] + dfs(edge_target))\n",
    "            funcall = [nodes[node], \"(\"]\n",
    "            for i, funcall_arg in enumerate(funcall_args):\n",
    "                if i != 0:\n",
    "                    funcall.append(\",\")\n",
    "                funcall.extend(funcall_arg)\n",
    "            funcall.append(\")\")\n",
    "            return funcall\n",
    "\n",
    "    return \" \".join(dfs(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec6c7fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./cogs/train.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "dev_df = pd.read_csv(\"./cogs/dev.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "test_df = pd.read_csv(\"./cogs/test.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "gen_df = pd.read_csv(\"./cogs/gen.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "\n",
    "train_df['LF'] = train_df[['LF']].apply(lambda x: cogs_lf_to_funcall(*x), axis=1,)\n",
    "dev_df['LF'] = dev_df[['LF']].apply(lambda x: cogs_lf_to_funcall(*x), axis=1,)\n",
    "test_df['LF'] = test_df[['LF']].apply(lambda x: cogs_lf_to_funcall(*x), axis=1,)\n",
    "gen_df['LF'] = gen_df[['LF']].apply(lambda x: cogs_lf_to_funcall(*x), axis=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72eb5e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_postfix = \"variable_free\"\n",
    "train_df.to_csv(f'./{dataset_postfix}/train.tsv', sep='\\t', index=False, header=False)\n",
    "dev_df.to_csv(f'./{dataset_postfix}/dev.tsv', sep='\\t', index=False, header=False)\n",
    "test_df.to_csv(f'./{dataset_postfix}/test.tsv', sep='\\t', index=False, header=False)\n",
    "gen_df.to_csv(f'./{dataset_postfix}/gen.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3cd798",
   "metadata": {},
   "source": [
    "### Participial Verb Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a4548149",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_pool = set()\n",
    "v_pool = set()\n",
    "\n",
    "def collect_nv(text, phi, _type):\n",
    "    if len(phi.split()) == 1:\n",
    "        return text, phi, _type\n",
    "    elif \"LAMBDA\" in phi:\n",
    "        return text, phi, _type\n",
    "    \n",
    "    # parse\n",
    "    text_split = text.split()\n",
    "    data = []    \n",
    "    conjs = re.split(r\"\\s*(?:AND|;)\\s*\", phi)\n",
    "    for conj in conjs: \n",
    "        if np_re.search(conj):\n",
    "            d = parse_np(conj)\n",
    "        elif pred_re.search(conj):\n",
    "            d = parse_pred(conj)\n",
    "        elif mod_re.search(conj):\n",
    "            d = parse_mod(conj)\n",
    "        else:\n",
    "            raise Exception(f\"Conjunct could not be parsed: {conj}\")\n",
    "        data.append(d)\n",
    "    \n",
    "    # collect\n",
    "    def_terms = []\n",
    "    role_terms = []\n",
    "    nmod_terms = []\n",
    "    role_set = set([])\n",
    "    for d in data:\n",
    "        if d['type'] == 'role':\n",
    "            v_pool.add(d['pred'])\n",
    "        elif d['type'] == 'np':\n",
    "            noun_pool.add(d['pred'])\n",
    "            \n",
    "    return text, phi, _type\n",
    "\n",
    "def translate_regular(text, phi, _type):\n",
    "    \n",
    "    if len(phi.split()) == 1:\n",
    "        return text, phi, _type\n",
    "    elif \"LAMBDA\" in phi:\n",
    "        return text, phi, _type\n",
    "    \n",
    "    # parse\n",
    "    text_split = text.split()\n",
    "    data = []    \n",
    "    conjs = re.split(r\"\\s*(?:AND|;)\\s*\", phi)\n",
    "    for conj in conjs: \n",
    "        if np_re.search(conj):\n",
    "            d = parse_np(conj)\n",
    "        elif pred_re.search(conj):\n",
    "            d = parse_pred(conj)\n",
    "        elif mod_re.search(conj):\n",
    "            d = parse_mod(conj)\n",
    "        else:\n",
    "            raise Exception(f\"Conjunct could not be parsed: {conj}\")\n",
    "        data.append(d)\n",
    "    \n",
    "    # collect\n",
    "    def_terms = []\n",
    "    role_terms = []\n",
    "    nmod_terms = []\n",
    "    for d in data:\n",
    "        if d['type'] == 'np':\n",
    "            if d['definiteness'] == '*':\n",
    "                def_terms += [f\"* {d['pred']} ( {d['entvar']} )\"]\n",
    "            else:\n",
    "                role_terms += [f\"{d['pred']} ( {d['entvar']} )\"]\n",
    "        if d['type'] == 'role':\n",
    "            role_terms += [f\"{d['pred']} . {d['role']} ( {d['eventvar']} , {d['entvar']} )\"]\n",
    "        elif d['type'] == 'mod':\n",
    "            assert \"x _\" in d['e1']\n",
    "            role_terms += [f\"{d['nppred']} . nmod . {d['pred']} ( {d['e1']} , {d['e2']} )\"]\n",
    "            \n",
    "    # sort def_terms\n",
    "    def_terms = [*set(def_terms)]\n",
    "    def_terms.sort(key = lambda x: int(x.split()[-2]))    \n",
    "    rest_terms = role_terms\n",
    "    \n",
    "    # combine\n",
    "    def_terms = \" ; \".join(def_terms)\n",
    "    if def_terms == \"\":\n",
    "        terms = \" AND \".join(rest_terms)\n",
    "    elif \" AND \".join(rest_terms) == \"\":\n",
    "        terms = def_terms\n",
    "    else:\n",
    "        terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "\n",
    "    return text, terms, _type\n",
    "\n",
    "def translate(text, phi, _type):\n",
    "    \n",
    "    if len(phi.split()) == 1:\n",
    "        return text, phi, _type\n",
    "    elif \"LAMBDA\" in phi:\n",
    "        return text, phi, _type\n",
    "    \n",
    "    # parse\n",
    "    text_split = text.split()\n",
    "    data = []    \n",
    "    conjs = re.split(r\"\\s*(?:AND|;)\\s*\", phi)\n",
    "    for conj in conjs: \n",
    "        if np_re.search(conj):\n",
    "            d = parse_np(conj)\n",
    "        elif pred_re.search(conj):\n",
    "            d = parse_pred(conj)\n",
    "        elif mod_re.search(conj):\n",
    "            d = parse_mod(conj)\n",
    "        else:\n",
    "            raise Exception(f\"Conjunct could not be parsed: {conj}\")\n",
    "        data.append(d)\n",
    "    \n",
    "    # collect\n",
    "    def_terms = []\n",
    "    role_terms = []\n",
    "    nmod_terms = []\n",
    "    role_set = set([])\n",
    "    object_entvar = None\n",
    "    for d in data:\n",
    "        if d['type'] == 'np':\n",
    "            if d['definiteness'] == '*':\n",
    "                def_terms += [f\"* {d['pred']} ( {d['entvar']} )\"]\n",
    "            else:\n",
    "                role_terms += [f\"{d['pred']} ( {d['entvar']} )\"]\n",
    "        if d['type'] == 'role':\n",
    "            role_terms += [f\"{d['pred']} . {d['role']} ( {d['eventvar']} , {d['entvar']} )\"]\n",
    "            if d['role'] == \"theme\":\n",
    "                object_entvar = d['entvar']\n",
    "                \n",
    "            role_set.add(d['role'])\n",
    "        elif d['type'] == 'mod':\n",
    "            assert \"x _\" in d['e1']\n",
    "            role_terms += [f\"{d['nppred']} . nmod . {d['pred']} ( {d['e1']} , {d['e2']} )\"]\n",
    "            \n",
    "    # sort def_terms\n",
    "    def_terms = [*set(def_terms)]\n",
    "    def_terms.sort(key = lambda x: int(x.split()[-2]))    \n",
    "    rest_terms = role_terms\n",
    "    \n",
    "    start_with_pp = False if text_split[0] in {'A', 'The'} else True\n",
    "    \n",
    "    if start_with_pp or (object_entvar is not None and \"x _ \" not in object_entvar):\n",
    "        # combine\n",
    "        def_terms = \" ; \".join(def_terms)\n",
    "        if def_terms == \"\":\n",
    "            terms = \" AND \".join(rest_terms)\n",
    "        elif \" AND \".join(rest_terms) == \"\":\n",
    "            terms = def_terms\n",
    "        else:\n",
    "            terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "        return text, terms, _type\n",
    "    \n",
    "    if random.random() >= participial_prob:\n",
    "        # combine\n",
    "        def_terms = \" ; \".join(def_terms)\n",
    "        if def_terms == \"\":\n",
    "            terms = \" AND \".join(rest_terms)\n",
    "        elif \" AND \".join(rest_terms) == \"\":\n",
    "            terms = def_terms\n",
    "        else:\n",
    "            terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "        return text, terms, _type\n",
    "    \n",
    "    # agent -> subject\n",
    "    # theme -> object\n",
    "    if \"theme\" in phi:\n",
    "        # only one object if it exists!\n",
    "        assert 1 == phi.split().count(\"theme\")\n",
    "        modify_subject = True if random.random() < 0.5 else False\n",
    "    else:\n",
    "        modify_subject = True\n",
    "    \n",
    "    # reindex!\n",
    "    def reindex_terms(terms, offset=1):\n",
    "        new_terms = []\n",
    "        for term in terms:\n",
    "            new_term = []\n",
    "            for t in term.split():\n",
    "                if t.isnumeric() and int(t) > offset:\n",
    "                    new_term += [str(int(t)+3)]\n",
    "                else:\n",
    "                    new_term += [t]\n",
    "            new_terms += [\" \".join(new_term)]\n",
    "        return new_terms\n",
    "    \n",
    "    # we need to decide whether to add to subj or obj.\n",
    "    if modify_subject:\n",
    "        subject = text_split[0] if start_with_pp else text_split[1]\n",
    "        verb_pp = random.choice(list(verb_pp_map.keys()))\n",
    "        noun_pp = random.choice(list(noun_pool))\n",
    "        articles = 'a' if random.random() < 0.5 else 'the'\n",
    "        text_pp = [verb_pp_map[verb_pp], articles, noun_pp]\n",
    "        if articles == 'a':\n",
    "            pp_np_term = f\"{noun_pp} ( x _ 3 )\" if start_with_pp else f\"{noun_pp} ( x _ 4 )\"\n",
    "        elif articles == 'the':\n",
    "            pp_np_term = f\"* {noun_pp} ( x _ 3 )\" if start_with_pp else f\"* {noun_pp} ( x _ 4 )\"\n",
    "        pp_role_term = f\"{subject} . nmod . {verb_pp} ( {subject} , x _ 3 )\" if start_with_pp else \\\n",
    "            f\"{subject} . nmod . {verb_pp} ( x _ 1 , x _ 4 )\"\n",
    "        \n",
    "        def_terms = reindex_terms(def_terms)\n",
    "        rest_terms = reindex_terms(rest_terms)\n",
    "        \n",
    "        object_pos = 1\n",
    "    else:\n",
    "        # locate the obj\n",
    "        assert \"x _ \" in object_entvar\n",
    "        object_pos = int(object_entvar.split()[-1])\n",
    "        _object = text_split[object_pos]\n",
    "        \n",
    "        verb_pp = random.choice(list(verb_pp_map.keys()))\n",
    "        noun_pp = random.choice(list(noun_pool))\n",
    "        articles = 'a' if random.random() < 0.5 else 'the'\n",
    "        text_pp = [verb_pp_map[verb_pp], articles, noun_pp]\n",
    "        if articles == 'a':\n",
    "            pp_np_term = f\"{noun_pp} ( x _ {object_pos+3} )\"\n",
    "        elif articles == 'the':\n",
    "            pp_np_term = f\"* {noun_pp} ( x _ {object_pos+3} )\"\n",
    "        pp_role_term = f\"{_object} . nmodpv . {verb_pp} ( x _ {object_pos} , x _ {object_pos+3} )\"\n",
    "\n",
    "        def_terms = reindex_terms(def_terms, object_pos)\n",
    "        rest_terms = reindex_terms(rest_terms, object_pos)\n",
    "        \n",
    "    if articles == 'the':\n",
    "        def_terms += [pp_np_term]\n",
    "        def_terms.sort(key = lambda x: int(x.split()[-2]))   \n",
    "        rest_terms = [pp_role_term] + rest_terms\n",
    "    else:\n",
    "        def_terms.sort(key = lambda x: int(x.split()[-2]))   \n",
    "        rest_terms = [pp_role_term, pp_np_term] + rest_terms\n",
    "\n",
    "    text_split = text_split[:object_pos+1] + text_pp + text_split[object_pos+1:]\n",
    "    new_text = \" \".join(text_split)\n",
    "    rest_terms.sort(key = lambda x: (int(x.split()[-2]) if x.split()[-5] == \"(\" else int(x.split()[-6]) if x.split()[-2].isnumeric() else int(x.split()[-4]), -1 if x.split()[-5] == \"(\" else int(x.split()[-2]) if x.split()[-2].isnumeric() else new_text.split().index(x.split()[-2])))  \n",
    "    # combine\n",
    "    def_terms = \" ; \".join(def_terms)\n",
    "    if def_terms == \"\":\n",
    "        terms = \" AND \".join(rest_terms)\n",
    "    elif \" AND \".join(rest_terms) == \"\":\n",
    "        terms = def_terms\n",
    "    else:\n",
    "        terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "        \n",
    "    return new_text, terms, \"participle_verb_phrase_hard\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "23fb2b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "participial_prob = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "14c9f375",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./cogs/train.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "dev_df = pd.read_csv(\"./cogs/dev.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "test_df = pd.read_csv(\"./cogs/test.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "gen_df = pd.read_csv(\"./cogs/gen.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "\n",
    "with open('./verb_pp_map.json', 'r') as openfile:\n",
    "    verb_pp_map = json.load(openfile)\n",
    "train_df[['sentence', 'LF', 'type']] = train_df[['sentence', 'LF', 'type']].apply(\n",
    "    lambda x: collect_nv(*x), axis=1, result_type='expand'\n",
    ")\n",
    "train_df[['sentence', 'LF', 'type']] = train_df[['sentence', 'LF', 'type']].apply(\n",
    "    lambda x: translate(*x), axis=1, result_type='expand'\n",
    ")\n",
    "dev_df[['sentence', 'LF', 'type']] = dev_df[['sentence', 'LF', 'type']].apply(lambda x: translate_regular(*x), axis=1, result_type='expand')\n",
    "test_df[['sentence', 'LF', 'type']] = test_df[['sentence', 'LF', 'type']].apply(lambda x: translate_regular(*x), axis=1, result_type='expand')\n",
    "gen_df[['sentence', 'LF', 'type']] = gen_df[['sentence', 'LF', 'type']].apply(lambda x: translate_regular(*x), axis=1, result_type='expand')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9d521dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_postfix = \"participle_verb_hard\"\n",
    "train_df.to_csv(f'./cogs_{dataset_postfix}/train.tsv', sep='\\t', index=False, header=False)\n",
    "dev_df.to_csv(f'./cogs_{dataset_postfix}/dev.tsv', sep='\\t', index=False, header=False)\n",
    "test_df.to_csv(f'./cogs_{dataset_postfix}/test.tsv', sep='\\t', index=False, header=False)\n",
    "gen_df.to_csv(f'./cogs_{dataset_postfix}/gen.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b66f15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
