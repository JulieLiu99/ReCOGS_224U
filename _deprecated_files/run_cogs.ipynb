{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e71e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b2062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    is_notebook = False\n",
    "    try:\n",
    "        cmd = argparse.ArgumentParser('The testing components of')\n",
    "        cmd.add_argument('--gpu', default=-1, type=int, help='use id of gpu, -1 if cpu.')\n",
    "        cmd.add_argument('--train_batch_size', default=128, type=int, help='training batch size')\n",
    "        cmd.add_argument('--eval_batch_size', default=128, type=int, help='training batch size')\n",
    "        cmd.add_argument('--lr', default=0.01, type=float, help='learning rate')\n",
    "        cmd.add_argument('--data_path', required=True, type=str, help='path to the training corpus')\n",
    "        cmd.add_argument(\n",
    "            '--encoder_config_path', \n",
    "            type=str, help='path to the encoder config'\n",
    "        )\n",
    "        cmd.add_argument(\n",
    "            '--decoder_config_path', \n",
    "            type=str, help='path to the decoder config'\n",
    "        )\n",
    "        cmd.add_argument('--max_seq_len', default=512, type=int)\n",
    "        cmd.add_argument('--seed', default=42, type=int)\n",
    "        cmd.add_argument('--gradient_accumulation_steps', default=1, type=int)\n",
    "        cmd.add_argument('--output_dir', required=True, type=str, help='save dir')\n",
    "        cmd.add_argument('--local_rank', default=-1, type=int, help='multi gpu training')\n",
    "        cmd.add_argument('--epochs', default=10, type=int, help='training epochs')\n",
    "        cmd.add_argument('--model_path', type=str, required=False, default=None)\n",
    "        cmd.add_argument('--warm_up', type=float, default=0.1)\n",
    "        cmd.add_argument('--is_wandb', default=False, action='store_true')\n",
    "        cmd.add_argument('--spanformer', default=False, action='store_true')\n",
    "        cmd.add_argument('--log_step', default=10, type=int)\n",
    "        cmd.add_argument('--valid_steps', default=500, type=int)\n",
    "        cmd.add_argument('--early_stopping', default=5, type=int)\n",
    "        cmd.add_argument('--device', default=\"cuda\", type=str, help='')\n",
    "        cmd.add_argument('--do_train', default=False, action='store_true')\n",
    "        cmd.add_argument('--do_eval', default=False, action='store_true')\n",
    "        cmd.add_argument('--do_test', default=False, action='store_true')\n",
    "        cmd.add_argument('--do_gen', default=False, action='store_true')\n",
    "        cmd.add_argument('--least_to_most', default=False, action='store_true')\n",
    "        cmd.add_argument('--use_glove', default=False, action='store_true')\n",
    "        cmd.add_argument('--eval_acc', default=False, action='store_true')\n",
    "        cmd.add_argument('--use_iiem', default=False, action='store_true')\n",
    "        cmd.add_argument('--output_json', default=False, action='store_true')\n",
    "        cmd.add_argument('--save_after_epoch', type=int, default=None)\n",
    "        cmd.add_argument('--lf', default=\"cogs\", type=str, help='')\n",
    "        cmd.add_argument('--model_name', default=\"cogs\", type=str, help='')\n",
    "        \n",
    "        args = cmd.parse_args(sys.argv[1:])\n",
    "    except:\n",
    "        # LSTM settings best: {batch = 512, lr = 8e-4, epoch = 200}\n",
    "        # Transformer settings best: {batch = 128, lr = 1e-4, epoch = 200}\n",
    "        is_notebook = True\n",
    "        parser = argparse.ArgumentParser()\n",
    "        args = parser.parse_args([])\n",
    "        args.gpu = 1\n",
    "        args.train_batch_size = 512\n",
    "        args.eval_batch_size = 256\n",
    "        args.gradient_accumulation_steps = 1\n",
    "        args.lr = 8e-4\n",
    "        args.data_path = \"./cogs_participle_verb/\"\n",
    "        args.model_data_path = \"./model/\"\n",
    "        args.encoder_config_path = None\n",
    "        args.decoder_config_path = None\n",
    "        args.max_seq_len = 512\n",
    "        args.seed = 77\n",
    "        args.output_dir = \"./results_cogs_notebook/\"\n",
    "        args.epochs = 200\n",
    "        args.warm_up = 0.1\n",
    "        args.is_wandb = False\n",
    "        args.log_step = 10\n",
    "        # args.valid_steps = 500 # -1 not do training eval!\n",
    "        args.valid_steps = -1\n",
    "        args.early_stopping = None # large == never early stop!\n",
    "        args.device = \"cuda:0\"\n",
    "        args.spanformer = False\n",
    "        args.model_path = None\n",
    "        args.do_train = True\n",
    "        args.do_eval = False\n",
    "        args.do_test = True\n",
    "        args.do_gen = True\n",
    "        args.least_to_most = False\n",
    "        args.use_glove = False\n",
    "        args.eval_acc = False\n",
    "        args.save_after_epoch = None\n",
    "        args.use_iiem = False\n",
    "        args.output_json = False\n",
    "        args.model_name = \"ende_lstm\"\n",
    "\n",
    "        print(\"Using in a notebook env.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdbdcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8363e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lf in [\n",
    "    \"cogs\",\n",
    "]:\n",
    "    for seed in [42, 66, 77, 88, 99]: # 42, 66, 77, 88, 99\n",
    "        set_seed(args.seed)\n",
    "        \n",
    "        data_variant = args.data_path.strip(\"./\")\n",
    "        args.lf = lf\n",
    "        args.seed = seed\n",
    "\n",
    "        model_name = args.model_name\n",
    "        run_name = f\"cogs_pipeline.model.{model_name}.lf.{args.lf}.glove.{args.use_glove}.seed.{args.seed}\"\n",
    "        if args.do_train == False:\n",
    "            args.model_path = f\"./{args.output_dir}/{run_name}/model-last/\"\n",
    "        \n",
    "        logger = logging.getLogger()\n",
    "        logger.setLevel(logging.INFO)\n",
    "        device = torch.device(args.device)\n",
    "        \n",
    "        encoder_config_filename = \"encoder_config_lstm.json\" if model_name == \"ende_lstm\" else \"encoder_config.json\"\n",
    "        decoder_config_filename = \"decoder_config_lstm.json\" if model_name == \"ende_lstm\" else \"decoder_config.json\"\n",
    "        \n",
    "        \n",
    "        if \"participle_verb\" in args.data_path:\n",
    "            config_encoder = AutoConfig.from_pretrained(\n",
    "                os.path.join(args.data_path, encoder_config_filename)\n",
    "            )\n",
    "            config_decoder = AutoConfig.from_pretrained(\n",
    "                    os.path.join(args.data_path, decoder_config_filename) if args.decoder_config_path is None else \\\n",
    "                        args.decoder_config_path\n",
    "            )\n",
    "        else:\n",
    "            config_encoder = AutoConfig.from_pretrained(\n",
    "                os.path.join(args.model_data_path, encoder_config_filename)\n",
    "            )\n",
    "            config_decoder = AutoConfig.from_pretrained(\n",
    "                    os.path.join(args.model_data_path, decoder_config_filename) if args.decoder_config_path is None else \\\n",
    "                        args.decoder_config_path\n",
    "            )\n",
    "\n",
    "        if \"participle_verb\" in args.data_path:\n",
    "            src_tokenizer = WordLevelTokenizer(\n",
    "                os.path.join(args.data_path, \"src_vocab.txt\"), \n",
    "                config_encoder,\n",
    "                max_seq_len=args.max_seq_len\n",
    "            )\n",
    "            tgt_tokenizer = WordLevelTokenizer(\n",
    "                os.path.join(args.data_path, \"tgt_vocab.txt\"), \n",
    "                config_decoder,\n",
    "                max_seq_len=args.max_seq_len\n",
    "            )  \n",
    "        else:\n",
    "            src_tokenizer = WordLevelTokenizer(\n",
    "                os.path.join(args.model_data_path, \"src_vocab.txt\"), \n",
    "                config_encoder,\n",
    "                max_seq_len=args.max_seq_len\n",
    "            )\n",
    "            tgt_tokenizer = WordLevelTokenizer(\n",
    "                os.path.join(args.model_data_path, \"tgt_vocab.txt\"), \n",
    "                config_decoder,\n",
    "                max_seq_len=args.max_seq_len\n",
    "            )\n",
    "\n",
    "        if args.least_to_most:\n",
    "            logging.info(\"Preparing training set to be least to most order.\")\n",
    "        train_dataset = COGSDataset(\n",
    "            cogs_path=args.data_path, \n",
    "            src_tokenizer=src_tokenizer, \n",
    "            tgt_tokenizer=tgt_tokenizer, \n",
    "            partition=find_partition_name(\"train\", args.lf),\n",
    "            least_to_most=args.least_to_most\n",
    "        )\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset, batch_size=args.train_batch_size, \n",
    "            sampler=SequentialSampler(train_dataset),\n",
    "            collate_fn=train_dataset.collate_batch\n",
    "        )\n",
    "\n",
    "        eval_dataset = COGSDataset(\n",
    "            cogs_path=args.data_path, \n",
    "            src_tokenizer=src_tokenizer, \n",
    "            tgt_tokenizer=tgt_tokenizer, \n",
    "            partition=find_partition_name(\"dev\", args.lf),\n",
    "        )\n",
    "        eval_dataloader = DataLoader(\n",
    "            eval_dataset, batch_size=args.eval_batch_size, \n",
    "            sampler=SequentialSampler(eval_dataset),\n",
    "            collate_fn=train_dataset.collate_batch\n",
    "        )\n",
    "\n",
    "        test_dataset = COGSDataset(\n",
    "            cogs_path=args.data_path, \n",
    "            src_tokenizer=src_tokenizer, \n",
    "            tgt_tokenizer=tgt_tokenizer, \n",
    "            partition=find_partition_name(\"test\", args.lf),\n",
    "        )\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset, batch_size=args.eval_batch_size, \n",
    "            sampler=SequentialSampler(test_dataset),\n",
    "            collate_fn=train_dataset.collate_batch\n",
    "        )\n",
    "\n",
    "        gen_dataset = COGSDataset(\n",
    "            cogs_path=args.data_path, \n",
    "            src_tokenizer=src_tokenizer, \n",
    "            tgt_tokenizer=tgt_tokenizer, \n",
    "            partition=find_partition_name(\"gen\", args.lf),\n",
    "        )\n",
    "        gen_dataloader = DataLoader(\n",
    "            gen_dataset, batch_size=args.eval_batch_size, \n",
    "            sampler=SequentialSampler(gen_dataset),\n",
    "            collate_fn=train_dataset.collate_batch\n",
    "        )\n",
    "        \n",
    "        if model_name == \"ende_transformer\":\n",
    "            logging.info(\"Baselining the Transformer Encoder-Decoder Model\")\n",
    "            model_config = EncoderDecoderConfig.from_encoder_decoder_configs(\n",
    "                config_encoder, config_decoder\n",
    "            )\n",
    "            model_config.decoder_start_token_id = config_encoder.bos_token_id\n",
    "            model_config.pad_token_id = config_encoder.pad_token_id\n",
    "            model_config.eos_token_id = config_encoder.eos_token_id\n",
    "            model = EncoderDecoderModel(config=model_config)\n",
    "        elif model_name == \"ende_lstm\":\n",
    "            logging.info(\"Baselining the LSTM Encoder-Decoder Model\")\n",
    "            model_config = EncoderDecoderConfig.from_encoder_decoder_configs(\n",
    "                config_encoder, config_decoder\n",
    "            )\n",
    "            model_config.decoder_start_token_id = config_encoder.bos_token_id\n",
    "            model_config.pad_token_id = config_encoder.pad_token_id\n",
    "            model_config.eos_token_id = config_encoder.eos_token_id\n",
    "            model = EncoderDecoderLSTMModel(config=model_config)\n",
    "            \n",
    "        if args.model_path is not None and model_name == \"ende_transformer\":\n",
    "            logging.info(\"Loading pretrained model.\")\n",
    "            model = model.from_pretrained(args.model_path)\n",
    "        elif args.model_path is not None and model_name == \"ende_lstm\":\n",
    "            logging.info(\"Loading pretrained model.\")\n",
    "            raw_weights = torch.load(os.path.join(args.model_path, 'pytorch_model.bin'))\n",
    "            model.load_state_dict(raw_weights)\n",
    "            \n",
    "        \n",
    "\n",
    "        if \"cuda:\" not in args.device:\n",
    "            n_gpu = torch.cuda.device_count()\n",
    "            logging.info(f'__Number CUDA Devices: {n_gpu}')\n",
    "        else:\n",
    "            n_gpu = 1\n",
    "            logging.info(f'__Number CUDA Devices: {n_gpu}')\n",
    "\n",
    "        if n_gpu > 1:\n",
    "            model = torch.nn.DataParallel(model)\n",
    "        _ = model.to(device)\n",
    "\n",
    "        t_total = int(len(train_dataloader) * args.epochs)\n",
    "\n",
    "        warm_up_steps = args.warm_up * t_total\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(), lr=args.lr\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warm_up_steps,\n",
    "                                                    num_training_steps=t_total)\n",
    "        is_master = True\n",
    "        apex_enable = False                                       \n",
    "        if not os.path.exists(args.output_dir) and is_master:\n",
    "            os.mkdir(args.output_dir)\n",
    "\n",
    "        os.environ[\"WANDB_PROJECT\"] = f\"COGS\"\n",
    "\n",
    "        output_dir = os.path.join(args.output_dir, run_name)\n",
    "        if args.is_wandb:\n",
    "            import wandb\n",
    "            run = wandb.init(\n",
    "                project=\"COGS-CKY-Transformer\", \n",
    "                entity=\"wuzhengx\",\n",
    "                name=run_name,\n",
    "            )\n",
    "            wandb.config.update(args)\n",
    "        if not os.path.exists(args.output_dir) and is_master:\n",
    "            os.mkdir(args.output_dir)\n",
    "\n",
    "        trainer = COGSTrainer(\n",
    "            model, device=device, \n",
    "            src_tokenizer=src_tokenizer, \n",
    "            tgt_tokenizer=tgt_tokenizer, \n",
    "            logger=logger,\n",
    "            is_master=is_master, \n",
    "            n_gpu=n_gpu,\n",
    "            is_wandb=args.is_wandb, \n",
    "            model_name=model_name,\n",
    "            eval_acc=args.eval_acc,\n",
    "        )\n",
    "        num_params = count_parameters(model)\n",
    "        logging.info(f'Number of model params: {num_params}')\n",
    "\n",
    "        if args.do_train:\n",
    "            logging.info(f\"OUTPUT DIR: {output_dir}\")\n",
    "            trainer.train(\n",
    "                train_dataloader, eval_dataloader,\n",
    "                optimizer, scheduler, \n",
    "                log_step=args.log_step, valid_steps=args.valid_steps,\n",
    "                output_dir=output_dir, epochs=args.epochs, \n",
    "                gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "                save_after_epoch=args.save_after_epoch,\n",
    "            )\n",
    "        \n",
    "        if args.do_test and not args.use_iiem:\n",
    "            trainer.model.eval()\n",
    "            epoch_iterator = tqdm(test_dataloader, desc=\"Iteration\", position=0, leave=True)\n",
    "            total_count = 0\n",
    "            correct_count = 0\n",
    "            for step, inputs in enumerate(epoch_iterator):\n",
    "                input_ids = inputs[\"input_ids\"].to(device)\n",
    "                attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "                labels = inputs[\"labels\"].to(device)\n",
    "                if model_name == \"ende_lstm\":\n",
    "                    outputs = trainer.model.generate(\n",
    "                        input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                    )\n",
    "                else:\n",
    "                    outputs = trainer.model.generate(\n",
    "                        input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        eos_token_id=model_config.eos_token_id,\n",
    "                        max_length=args.max_seq_len,\n",
    "                    )\n",
    "                decoded_preds = tgt_tokenizer.batch_decode(outputs)\n",
    "                decoded_labels = tgt_tokenizer.batch_decode(labels)\n",
    "\n",
    "                for i in range(len(decoded_preds)):\n",
    "                    if decoded_preds[i] == decoded_labels[i]:\n",
    "                        correct_count += 1\n",
    "                    else:\n",
    "                        print(decoded_preds[i])\n",
    "                        print(decoded_labels[i])\n",
    "                    total_count += 1\n",
    "                current_acc = round(correct_count/total_count, 2)\n",
    "                epoch_iterator.set_postfix({'acc': current_acc})\n",
    "            test_acc = current_acc\n",
    "\n",
    "        if args.do_gen and not args.use_iiem:\n",
    "            per_cat_eval = {}\n",
    "            for cat in set(gen_dataset.eval_cat):\n",
    "                per_cat_eval[cat] = [0, 0] # correct, total\n",
    "            trainer.model.eval()\n",
    "            epoch_iterator = tqdm(gen_dataloader, desc=\"Iteration\", position=0, leave=True)\n",
    "            total_count = 0\n",
    "            correct_count = 0\n",
    "            for step, inputs in enumerate(epoch_iterator):\n",
    "                input_ids = inputs[\"input_ids\"].to(device)\n",
    "                attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "                labels = inputs[\"labels\"].to(device)\n",
    "                if model_name == \"ende_lstm\":\n",
    "                    outputs = trainer.model.generate(\n",
    "                        input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                    )\n",
    "                else:\n",
    "                    outputs = trainer.model.generate(\n",
    "                        input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        eos_token_id=model_config.eos_token_id,\n",
    "                        max_length=args.max_seq_len,\n",
    "                    )\n",
    "                decoded_preds = tgt_tokenizer.batch_decode(outputs)\n",
    "                decoded_labels = tgt_tokenizer.batch_decode(labels)\n",
    "\n",
    "                input_labels = src_tokenizer.batch_decode(input_ids)\n",
    "                for i in range(len(decoded_preds)):\n",
    "                    cat = gen_dataset.eval_cat[total_count]\n",
    "                    if decoded_preds[i] == decoded_labels[i]:\n",
    "                        correct_count += 1\n",
    "                        per_cat_eval[cat][0] += 1\n",
    "                    else:\n",
    "                        if cat == \"prim_to_obj_proper\":\n",
    "                            pass\n",
    "                            # print(\"input: \", input_labels[i])\n",
    "                            # print(\"pred: \", decoded_preds[i])\n",
    "                            # print(\"actual: \", decoded_labels[i])\n",
    "                            # print(\"cat: \", cat)\n",
    "                            # print()\n",
    "                    total_count += 1\n",
    "                    per_cat_eval[cat][1] += 1\n",
    "                current_acc = correct_count/total_count\n",
    "                epoch_iterator.set_postfix({'acc': current_acc})\n",
    "\n",
    "            struct_pp_acc = 0\n",
    "            struct_cp_acc = 0\n",
    "            struct_obj_subj_acc = 0\n",
    "\n",
    "            lex_acc = 0\n",
    "            lex_count = 0\n",
    "            for k, v in per_cat_eval.items():\n",
    "                if k  == \"pp_recursion\":\n",
    "                    struct_pp_acc = 100 * v[0]/v[1]\n",
    "                elif k  == \"cp_recursion\":\n",
    "                    struct_cp_acc = 100 * v[0]/v[1]\n",
    "                elif k  == \"obj_pp_to_subj_pp\":\n",
    "                    struct_obj_subj_acc = 100 * v[0]/v[1]\n",
    "                elif k  == \"subj_to_obj_proper\":\n",
    "                    subj_to_obj_proper_acc = 100 * v[0]/v[1]\n",
    "                elif k  == \"prim_to_obj_proper\":\n",
    "                    prim_to_obj_proper_acc = 100 * v[0]/v[1]\n",
    "                elif k  == \"prim_to_subj_proper\": \n",
    "                    prim_to_subj_proper_acc = 100 * v[0]/v[1]\n",
    "                else:\n",
    "                    lex_acc += v[0]\n",
    "                    lex_count += v[1]\n",
    "            lex_acc /= lex_count\n",
    "            lex_acc *= 100\n",
    "            current_acc *= 100\n",
    "\n",
    "            print(f\"obj_pp_to_subj_pp: {struct_obj_subj_acc}\")\n",
    "            print(f\"cp_recursion: {struct_cp_acc}\")\n",
    "            print(f\"pp_recursion: {struct_pp_acc}\")\n",
    "            print(f\"subj_to_obj_proper: {subj_to_obj_proper_acc}\")\n",
    "            print(f\"prim_to_obj_proper: {prim_to_obj_proper_acc}\")\n",
    "            print(f\"prim_to_subj_proper: {prim_to_subj_proper_acc}\")\n",
    "            print(f\"LEX: {lex_acc}\")\n",
    "            print(f\"OVERALL: {current_acc}\")\n",
    "\n",
    "            results[(seed, lf)] = {\n",
    "                \"obj_pp_to_subj_pp\" : struct_obj_subj_acc,\n",
    "                \"cp_recursion\" : struct_cp_acc,\n",
    "                \"pp_recursion\" : struct_pp_acc,\n",
    "                \"subj_to_obj_proper\" : subj_to_obj_proper_acc,\n",
    "                \"prim_to_obj_proper\" : prim_to_obj_proper_acc,\n",
    "                \"prim_to_subj_proper\" : prim_to_subj_proper_acc,\n",
    "                \"lex_acc\" : lex_acc,\n",
    "                \"overall_acc\" : current_acc,\n",
    "                \"test_acc\" : test_acc\n",
    "            }\n",
    "\n",
    "        if args.do_test and args.use_iiem:\n",
    "            trainer.model.eval()\n",
    "            epoch_iterator = tqdm(test_dataloader, desc=\"Iteration\", position=0, leave=True)\n",
    "            total_count = 0\n",
    "            correct_count = 0\n",
    "            for step, inputs in enumerate(epoch_iterator):\n",
    "                input_ids = inputs[\"input_ids\"].to(device)\n",
    "                attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "                labels = inputs[\"labels\"].to(device)\n",
    "                if model_name == \"ende_lstm\":\n",
    "                    outputs = trainer.model.generate(\n",
    "                        input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                    )\n",
    "                else:\n",
    "                    outputs = trainer.model.generate(\n",
    "                        input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        eos_token_id=model_config.eos_token_id,\n",
    "                        max_length=args.max_seq_len,\n",
    "                    )\n",
    "                decoded_preds = tgt_tokenizer.batch_decode(outputs)\n",
    "                decoded_labels = tgt_tokenizer.batch_decode(labels)\n",
    "\n",
    "                for i in range(len(decoded_preds)):\n",
    "\n",
    "                    index_mapping = {}\n",
    "                    current_idx = 0\n",
    "                    for t in decoded_labels[i].split():\n",
    "                        if t.isnumeric():\n",
    "                            if int(t) not in index_mapping:\n",
    "                                index_mapping[int(t)] = current_idx\n",
    "                                current_idx += 1\n",
    "                    decoded_labels_ii = []\n",
    "                    for t in decoded_labels[i].split():\n",
    "                        if t.isnumeric():\n",
    "                            decoded_labels_ii += [str(index_mapping[int(t)])]\n",
    "                        else:\n",
    "                            decoded_labels_ii += [t]\n",
    "\n",
    "                    index_mapping = {}\n",
    "                    current_idx = 0\n",
    "                    for t in decoded_preds[i].split():\n",
    "                        if t.isnumeric():\n",
    "                            if int(t) not in index_mapping:\n",
    "                                index_mapping[int(t)] = current_idx\n",
    "                                current_idx += 1\n",
    "                    decoded_preds_ii = []\n",
    "                    for t in decoded_preds[i].split():\n",
    "                        if t.isnumeric():\n",
    "                            decoded_preds_ii += [str(index_mapping[int(t)])]\n",
    "                        else:\n",
    "                            decoded_preds_ii += [t]\n",
    "\n",
    "\n",
    "                    decoded_labels_ii_str = \" \".join(decoded_labels_ii)\n",
    "                    decoded_preds_ii_str = \" \".join(decoded_preds_ii)\n",
    "\n",
    "                    if decoded_preds_ii_str == decoded_labels_ii_str:\n",
    "                        correct_count += 1\n",
    "                    else:\n",
    "                        print(decoded_labels_ii_str)\n",
    "                        print(decoded_preds_ii_str)\n",
    "\n",
    "                    total_count += 1\n",
    "                current_acc = round(correct_count/total_count, 2)\n",
    "                epoch_iterator.set_postfix({'acc': current_acc})\n",
    "            test_acc = current_acc\n",
    "            \n",
    "        if args.do_gen and args.use_iiem:\n",
    "            per_cat_eval = {}\n",
    "            for cat in set(gen_dataset.eval_cat):\n",
    "                per_cat_eval[cat] = [0, 0] # correct, total\n",
    "            trainer.model.eval()\n",
    "            epoch_iterator = tqdm(gen_dataloader, desc=\"Iteration\", position=0, leave=True)\n",
    "            total_count = 0\n",
    "            correct_count = 0\n",
    "            for step, inputs in enumerate(epoch_iterator):\n",
    "                input_ids = inputs[\"input_ids\"].to(device)\n",
    "                attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "                labels = inputs[\"labels\"].to(device)\n",
    "                if model_name == \"ende_lstm\":\n",
    "                    outputs = trainer.model.generate(\n",
    "                        input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                    )\n",
    "                else:\n",
    "                    outputs = trainer.model.generate(\n",
    "                        input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        eos_token_id=model_config.eos_token_id,\n",
    "                        max_length=args.max_seq_len,\n",
    "                    )\n",
    "                decoded_preds = tgt_tokenizer.batch_decode(outputs)\n",
    "                decoded_labels = tgt_tokenizer.batch_decode(labels)\n",
    "\n",
    "                input_labels = src_tokenizer.batch_decode(input_ids)\n",
    "                for i in range(len(decoded_preds)):\n",
    "\n",
    "                    index_mapping = {}\n",
    "                    current_idx = 0\n",
    "                    for t in decoded_labels[i].split():\n",
    "                        if t.isnumeric():\n",
    "                            if int(t) not in index_mapping:\n",
    "                                index_mapping[int(t)] = current_idx\n",
    "                                current_idx += 1\n",
    "                    decoded_labels_ii = []\n",
    "                    for t in decoded_labels[i].split():\n",
    "                        if t.isnumeric():\n",
    "                            decoded_labels_ii += [str(index_mapping[int(t)])]\n",
    "                        else:\n",
    "                            decoded_labels_ii += [t]\n",
    "\n",
    "                    index_mapping = {}\n",
    "                    current_idx = 0\n",
    "                    for t in decoded_preds[i].split():\n",
    "                        if t.isnumeric():\n",
    "                            if int(t) not in index_mapping:\n",
    "                                index_mapping[int(t)] = current_idx\n",
    "                                current_idx += 1\n",
    "                    decoded_preds_ii = []\n",
    "                    for t in decoded_preds[i].split():\n",
    "                        if t.isnumeric():\n",
    "                            decoded_preds_ii += [str(index_mapping[int(t)])]\n",
    "                        else:\n",
    "                            decoded_preds_ii += [t]\n",
    "\n",
    "\n",
    "                    decoded_labels_ii_str = \" \".join(decoded_labels_ii)\n",
    "                    decoded_preds_ii_str = \" \".join(decoded_preds_ii)\n",
    "\n",
    "                    cat = gen_dataset.eval_cat[total_count]\n",
    "                    if decoded_preds_ii_str == decoded_labels_ii_str:\n",
    "                        correct_count += 1\n",
    "                        per_cat_eval[cat][0] += 1\n",
    "                        if cat == \"obj_pp_to_subj_pp\":\n",
    "                            pass\n",
    "                    else:\n",
    "                        if cat == \"obj_pp_to_subj_pp\":\n",
    "                            # pass\n",
    "                            print(\"input: \", input_labels[i])\n",
    "                            print(\"pred: \", decoded_preds_ii_str)\n",
    "                            print(\"actual: \", decoded_labels_ii_str)\n",
    "                            print(\"cat: \", cat)\n",
    "                            print()\n",
    "                    total_count += 1\n",
    "                    per_cat_eval[cat][1] += 1\n",
    "                current_acc = correct_count/total_count\n",
    "                epoch_iterator.set_postfix({'acc': current_acc})\n",
    "\n",
    "            struct_pp_acc = 0\n",
    "            struct_cp_acc = 0\n",
    "            struct_obj_subj_acc = 0\n",
    "\n",
    "            lex_acc = 0\n",
    "            lex_count = 0\n",
    "            for k, v in per_cat_eval.items():\n",
    "                if k  == \"pp_recursion\":\n",
    "                    struct_pp_acc = 100 * v[0]/v[1]\n",
    "                elif k  == \"cp_recursion\":\n",
    "                    struct_cp_acc = 100 * v[0]/v[1]\n",
    "                elif k  == \"obj_pp_to_subj_pp\":\n",
    "                    struct_obj_subj_acc = 100 * v[0]/v[1]\n",
    "                elif k  == \"subj_to_obj_proper\":\n",
    "                    subj_to_obj_proper_acc = 100 * v[0]/v[1]\n",
    "                elif k  == \"prim_to_obj_proper\":\n",
    "                    prim_to_obj_proper_acc = 100 * v[0]/v[1]\n",
    "                elif k  == \"prim_to_subj_proper\":\n",
    "                    prim_to_subj_proper_acc = 100 * v[0]/v[1]\n",
    "                else:\n",
    "                    lex_acc += v[0]\n",
    "                    lex_count += v[1]\n",
    "            lex_acc /= lex_count\n",
    "            lex_acc *= 100\n",
    "            current_acc *= 100\n",
    "\n",
    "            print(f\"obj_pp_to_subj_pp: {struct_obj_subj_acc}\")\n",
    "            print(f\"cp_recursion: {struct_cp_acc}\")\n",
    "            print(f\"pp_recursion: {struct_pp_acc}\")\n",
    "            print(f\"subj_to_obj_proper: {subj_to_obj_proper_acc}\")\n",
    "            print(f\"prim_to_obj_proper: {prim_to_obj_proper_acc}\")\n",
    "            print(f\"prim_to_subj_proper: {prim_to_subj_proper_acc}\")\n",
    "            print(f\"LEX: {lex_acc}\")\n",
    "            print(f\"OVERALL: {current_acc}\")\n",
    "\n",
    "            results[(seed, data_variant+\"_\"+lf)] = {\n",
    "                \"obj_pp_to_subj_pp\" : struct_obj_subj_acc,\n",
    "                \"cp_recursion\" : struct_cp_acc,\n",
    "                \"pp_recursion\" : struct_pp_acc,\n",
    "                \"subj_to_obj_proper\" : subj_to_obj_proper_acc,\n",
    "                \"prim_to_obj_proper\" : prim_to_obj_proper_acc,\n",
    "                \"prim_to_subj_proper\" : prim_to_subj_proper_acc,\n",
    "                \"lex_acc\" : lex_acc,\n",
    "                \"overall_acc\" : current_acc,\n",
    "                \"test_acc\" : test_acc\n",
    "            }\n",
    "\n",
    "    if args.output_json:\n",
    "        import json\n",
    "        json_object = json.dumps(results, indent=4)\n",
    "        data_filename = data_variant+\"_\"+lf\n",
    "        with open(f\"{data_filename}.json\", \"w\") as outfile:\n",
    "            outfile.write(json_object)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
