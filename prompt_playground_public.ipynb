{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c920f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "import openai\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_metric, load_from_disk\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "import itertools\n",
    "import random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb151b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_generated_answer(tokens, newline=\"\\n\" ): \n",
    "    \"\"\"Our LMs tend to insert initial newline characters before\n",
    "    they begin generating text. This function ensures that we \n",
    "    properly capture the true first line as the answer while\n",
    "    also ensuring that token probabilities are aligned.\"\"\"        \n",
    "    answer_token_indices = []\n",
    "    char_seen = False            \n",
    "    for i, tok in enumerate(tokens):\n",
    "        # This is the main condition: a newline that isn't an initial\n",
    "        # string of newlines:\n",
    "        if tok == newline and char_seen:\n",
    "            break\n",
    "        # Keep the initial newlines for consistency:\n",
    "        elif tok == newline and not char_seen:\n",
    "            answer_token_indices.append(i)\n",
    "        # Proper tokens:\n",
    "        elif tok != newline:\n",
    "            char_seen = True\n",
    "            answer_token_indices.append(i)\n",
    "    return answer_token_indices \n",
    "\n",
    "def few_shot_sample_random( \n",
    "    ex,\n",
    "    df,\n",
    "    n=50,\n",
    "):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    ex : Pandas DataFrame row\n",
    "        Single example for which we want to find few-shot samples for.\n",
    "    df : Pandas DataFrame\n",
    "        Counterfactual dataframe, from which to choose the samples. \n",
    "    n : int, default 2\n",
    "        Number of few-shot samples to generate. \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Pandas DataFrame\n",
    "        Returns `n` sample rows, chosen randomly from a sample pool\n",
    "        in `df` filtered by the boolean flags.\n",
    "    \"\"\"\n",
    "    \n",
    "    filtered_df = df # no filtering\n",
    "    if n < len(filtered_df):\n",
    "        return filtered_df.sample(n)\n",
    "    return filtered_df\n",
    "\n",
    "def few_shot_sample_keyword( \n",
    "    ex,\n",
    "    df,\n",
    "    n=50,\n",
    "):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    ex : Pandas DataFrame row\n",
    "        Single example for which we want to find few-shot samples for.\n",
    "    df : Pandas DataFrame\n",
    "        Counterfactual dataframe, from which to choose the samples. \n",
    "    n : int, default 2\n",
    "        Number of few-shot samples to generate. \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Pandas DataFrame\n",
    "        Returns `n` sample rows, chosen randomly from a sample pool\n",
    "        in `df` filtered by the boolean flags.\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence = ex[['sentence']].iloc[0]\n",
    "    \n",
    "    keywords = set([])\n",
    "    for w in sentence.split():\n",
    "        if w not in {'The', 'A', 'a', 'the', '.'}:\n",
    "            keywords.add(w)\n",
    "    \n",
    "    keyword_dfs = []\n",
    "    nonkeyword_dfs = []\n",
    "    for k in list(keywords):\n",
    "        contain_values = df[df['sentence'].str.contains(k)]\n",
    "        if len(contain_values.index) > 0:\n",
    "            keyword_dfs += [contain_values.sample(min([len(contain_values), 5]))]\n",
    "        \n",
    "        noncontain_values = df[~df['sentence'].str.contains(k)]\n",
    "        nonkeyword_dfs += [noncontain_values]\n",
    "        \n",
    "    filtered_df = pd.concat(keyword_dfs).drop_duplicates(subset=['sentence'])\n",
    "    nonkeyword_dfs = pd.concat(nonkeyword_dfs).drop_duplicates(subset=['sentence'])\n",
    "\n",
    "    if n < len(filtered_df):\n",
    "        return filtered_df.sample(n)\n",
    "    filtered_df = pd.concat([filtered_df, nonkeyword_dfs.sample(n - len(filtered_df))])\n",
    "    assert len(filtered_df) == n\n",
    "    return filtered_df\n",
    "\n",
    "def generate_prompt(ex, few_shots, start_prompt=True, joiner='\\n\\n'):\n",
    "    \"\"\"\n",
    "    Generates prompt for few-shot learning. An example:\n",
    "    \n",
    "    Please follow the instructions to manipulate the characters of the \n",
    "    INPUT string and generate the desired OUTPUT string.\n",
    "    \n",
    "    INPUT: tuo\n",
    "    \n",
    "    OUTPUT: out\n",
    "    \n",
    "    [... total k-shots of demonstrations ...]\n",
    "    \n",
    "    INPUT: nethgirf\n",
    "    \n",
    "    OUTPUT:\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ex : Pandas DataFrame row\n",
    "        Single example for which we want to generate a counterfactual. \n",
    "    few_shots : Pandas DataFrame\n",
    "        Counterfactual examples, as chosen by `few_shot_sample`\n",
    "    start_prompt : bool, default True\n",
    "        Whether or not to include a prompt at the beginning. \n",
    "    clue_type : str, default None\n",
    "        Whether to describe the clue type in the prompt\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    string\n",
    "        Few-shot prompt to provide to a language-generating model. \n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = []\n",
    "    \n",
    "    if start_prompt:\n",
    "        starter = \"Please translate a sentence into a logical form.\"\n",
    "        prompt += [starter]\n",
    "        \n",
    "    for i in range(len(few_shots)):\n",
    "        input_sentence = few_shots.iloc[i][0]\n",
    "        output_sentence = few_shots.iloc[i][1]\n",
    "        prompt += [f\"INPUT:{input_sentence}\"]\n",
    "        prompt += [f\"OUTPUT:{output_sentence}\"]\n",
    "    \n",
    "    test_sentence = ex[0]\n",
    "    target_text = ex[1]\n",
    "    \n",
    "    prompt += [f'INPUT:{test_sentence}']\n",
    "    prompt += ['OUTPUT:']\n",
    "\n",
    "    # clean out any newlines within sentences\n",
    "    prompt = [' '.join(l.split('\\n')) for l in prompt]\n",
    "    return joiner.join(prompt), target_text\n",
    "\n",
    "\n",
    "def generate_prompts(\n",
    "    sample_df, train_df, nshot, \n",
    "    start_prompt=True,\n",
    "    **fs_kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates prompts for all samples in `sample_df`, where the few-shot\n",
    "    samples are taken from `train_df`.\n",
    "    \"\"\"\n",
    "    def gp(ex):\n",
    "        few_shots = few_shot_sample_keyword(ex, train_df, n=nshot, **fs_kwargs)\n",
    "        return generate_prompt(\n",
    "            ex, few_shots, \n",
    "            start_prompt=start_prompt,\n",
    "        )\n",
    "    \n",
    "    return list(sample_df.apply(gp, axis=1))\n",
    "\n",
    "def run_gpt3(prompts, keys_batch=None, engine=\"text-curie-001\", model=None, temperature=0.0, max_tokens=64, **gpt3_kwargs):\n",
    "    \"\"\"\n",
    "    Runs GPT-3 on a list of prompts.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prompts : iterable of str\n",
    "    engine : str  \n",
    "        https://beta.openai.com/docs/engines/gpt-3                \n",
    "    temperature : float\n",
    "        It seems best to set it high for this task!\n",
    "    max_tokens: int\n",
    "        Limits how many tokens the model is asked to generate.\n",
    "        \n",
    "    For information about values for `gpt3_kwargs`, see\n",
    "    \n",
    "    https://beta.openai.com/docs/api-reference/completions\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of dicts   \n",
    "    \"\"\"\n",
    "    OPEN_AI_API_KEY = '<YOUR_KEY_GOES_HERE>'\n",
    "    openai.api_key = OPEN_AI_API_KEY\n",
    "\n",
    "    assert (engine is not None) or (model is not None), 'Please provide an engine or a finetuned model id.'\n",
    "\n",
    "    # go with pretrained model if provided, else use engine\n",
    "    if model is not None:\n",
    "        gpt3_kwargs['model'] = model\n",
    "    else:\n",
    "        gpt3_kwargs['engine'] = engine\n",
    "        \n",
    "    response = openai.Completion.create(\n",
    "        prompt=prompts,\n",
    "        temperature=temperature,\n",
    "        echo=False,   # This function will not work\n",
    "        logprobs=1,   # properly if any of these\n",
    "        n=1,          # are changed!\n",
    "        max_tokens=max_tokens,\n",
    "        **gpt3_kwargs)\n",
    "    \n",
    "    # From here, we parse each example to get the values\n",
    "    # we need:\n",
    "    data = []\n",
    "    prompt_index = 0\n",
    "    for ex, prompt in zip(response[\"choices\"], prompts):\n",
    "        tokens = ex[\"logprobs\"][\"tokens\"]\n",
    "        logprobs = ex[\"logprobs\"][\"token_logprobs\"]        \n",
    "        probs = list(np.exp(logprobs))\n",
    "        if \"<|endoftext|>\" in tokens:\n",
    "            end_i = tokens.index(\"<|endoftext|>\")\n",
    "            tokens = tokens[ : end_i]  # This leaves off the \"<|endoftext|>\"\n",
    "            probs = probs[ : end_i]    # token -- perhaps dubious.\n",
    "        ans_indices = _find_generated_answer(tokens)\n",
    "        answer_tokens = [tokens[i] for i in ans_indices]\n",
    "        answer_probs = [probs[i] for i in ans_indices]\n",
    "        answer = \"\".join(answer_tokens)        \n",
    "        data.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"generated_text\": ex[\"text\"],\n",
    "            \"generated_tokens\": tokens,\n",
    "            \"generated_probs\": probs,\n",
    "            \"generated_answer\": answer,\n",
    "            \"generated_answer_tokens\": answer_tokens,\n",
    "            \"generated_answer_probs\": answer_probs, \n",
    "            \"target_answer\": keys_batch[prompt_index] if keys_batch is not None else None})\n",
    "        prompt_index += 1\n",
    "    return data\n",
    "\n",
    "def run_gpt3_experiment(\n",
    "    name,\n",
    "    train_df, \n",
    "    dev_df, \n",
    "    nshot=0, \n",
    "    engine='curie', \n",
    "    start_prompt=True, \n",
    "    temperature=0.0, \n",
    "    batch_size=20,\n",
    "    ft_kwargs={},\n",
    "    fs_kwargs={},\n",
    "    **gpt3_kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Uses GPT-3 to generate counterfactuals on `dev_df`, using `train_df` to either\n",
    "    finetune the model or to sample few-shot examples. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    name : string\n",
    "        Name of the experiment. GPT-3 outputs will be saved to a json file with \n",
    "        the provided name.\n",
    "    train_df : Pandas DataFrame\n",
    "        Training data, formatted as output of `reframe_counterfactual`.\n",
    "    dev_df : Pandas DataFrame\n",
    "        Validation data, formatted as output of `reframe_counterfactual`.        \n",
    "    engine : string, default 'curie'\n",
    "        One of 'ada', 'babbage', 'curie', or 'davinci' (will automatically\n",
    "        take the 'text' option of the model)\n",
    "        https://beta.openai.com/docs/engines/gpt-3\n",
    "    start_prompt : bool, default True\n",
    "        Whether prompts should contain an initial prefix.\n",
    "    clue_type : bool, default True\n",
    "        Whether prompts should contain an initial prefix of the clue type.\n",
    "    temperature : float, default 0.7\n",
    "        Temperature of GPT-3 model (higher -> more creative)\n",
    "    batch_size : int, default 20\n",
    "        Batch sizes for GPT-3 API (20 is largest for free acount)\n",
    "    ft_kwargs : dict\n",
    "        Keyword arguments for finetuning, `finetune_gpt3`.\n",
    "    fs_kwargs : dict\n",
    "        Keyword arguments for few-shot generation, `few_shot_sample`.\n",
    "    gpt3_kwargs \n",
    "        Keyword arguments for the GPT-3 model, `run_gpt3`\n",
    "        https://beta.openai.com/docs/api-reference/fine-tunes\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of dicts\n",
    "        See `run_gpt3`.\n",
    "    \"\"\"\n",
    "    \n",
    "    if os.path.isfile(os.path.join(\"./openai-results\", f'{name}.json')):\n",
    "        print(\"Found existing file. Skipping the experiment ...\")\n",
    "        return None\n",
    "    \n",
    "    model = None\n",
    "    \n",
    "    prompts_with_keys = generate_prompts(\n",
    "        dev_df, train_df=train_df, nshot=nshot, \n",
    "        start_prompt=start_prompt,\n",
    "        **fs_kwargs\n",
    "    )\n",
    "    \n",
    "    prompts = [p_k[0] for p_k in prompts_with_keys]\n",
    "    \n",
    "    print(\"NUM OF PROMPTS: \", len(prompts))\n",
    "    keys = [p_k[1] for p_k in prompts_with_keys]\n",
    "    output = []\n",
    "    for b in range(0, len(prompts), batch_size):\n",
    "        batch = prompts[b:b + batch_size]\n",
    "        keys_batch = keys[b:b + batch_size]\n",
    "        output += run_gpt3(\n",
    "            batch, keys_batch, engine=engine, model=model, temperature=temperature, **gpt3_kwargs\n",
    "        )\n",
    "    with open(os.path.join(\"./openai-results\", f'{name}.json'), 'w') as f:\n",
    "        json.dump(output, f)\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b258dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../cogs/train.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "train_df = train_df[train_df['LF'].str.contains(\"LAMBDA\")==False]\n",
    "dev_df = pd.read_csv(\"../cogs/dev.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "test_df = pd.read_csv(\"../cogs/test.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "gen_df = pd.read_csv(\"../cogs/gen.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "results = []\n",
    "LIMIT_TEST_N = 1000\n",
    "seed = 42\n",
    "\n",
    "engines = [\n",
    "    \"text-davinci-003\",\n",
    "    \"text-davinci-002\",\n",
    "    \"text-curie-001\",\n",
    "    \"text-babbage-001\",\n",
    "    \"text-ada-001\",\n",
    "]\n",
    "test_splits = ['test', 'gen']\n",
    "for test_split in test_splits:\n",
    "    for engine in engines:\n",
    "        if \"davinci\" in engine:\n",
    "            nshot = 40\n",
    "        else:\n",
    "            nshot = 20\n",
    "        \n",
    "        _ = random.seed(seed)\n",
    "        _ = np.random.seed(seed)\n",
    "        _ = torch.manual_seed(seed)\n",
    "        if test_split == 'test':\n",
    "            eval_df = test_df\n",
    "        else:\n",
    "            eval_df = gen_df\n",
    "        run_name = f\"gpt3.{engine}.nshot.{nshot}.split.{test_split}.seed.{seed}\"\n",
    "        print(f\"RUNNING: {run_name}\")\n",
    "        # eval\n",
    "        _ = run_gpt3_experiment(\n",
    "            run_name, \n",
    "            train_df, \n",
    "            eval_df.sample(LIMIT_TEST_N), \n",
    "            nshot=nshot, \n",
    "            engine=engine, \n",
    "            start_prompt=True, \n",
    "            temperature=0.0, \n",
    "            batch_size=20,\n",
    "        )\n",
    "        print(f\"ANALYZING: {run_name}\")\n",
    "        output_parent_dir = \"openai-results\"\n",
    "        output_filename = f\"./{output_parent_dir}/{run_name}.json\"\n",
    "        output = json.load(open(output_filename))\n",
    "        total_count = 0\n",
    "        correct_count = 0\n",
    "        for ex in output:\n",
    "            total_count += 1\n",
    "            if ex[\"target_answer\"] == ex[\"generated_answer\"]:\n",
    "                correct_count += 1\n",
    "        exact_match = correct_count/total_count\n",
    "        print(f\"Exact Match: {exact_match}\")\n",
    "        results += [[engine, test_split, seed, exact_match]]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5194f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_train_df = pd.read_csv(\"../variable_free/train.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "free_train_df = free_train_df[free_train_df['LF'].str.contains(\"LAMBDA\")==False]\n",
    "free_dev_df = pd.read_csv(\"../variable_free/dev.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "free_test_df = pd.read_csv(\"../variable_free/test.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "free_gen_df = pd.read_csv(\"../variable_free/gen.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "free_results = []\n",
    "LIMIT_TEST_N = 1000\n",
    "seed = 42\n",
    "\n",
    "engines = [\n",
    "    \"text-davinci-003\",\n",
    "    \"text-davinci-002\",\n",
    "    \"text-curie-001\",\n",
    "    \"text-babbage-001\",\n",
    "    \"text-ada-001\",\n",
    "]\n",
    "test_splits = ['free_test', 'free_gen']\n",
    "for test_split in test_splits:\n",
    "    for engine in engines:\n",
    "        if \"davinci\" in engine:\n",
    "            nshot = 40\n",
    "        else:\n",
    "            nshot = 20\n",
    "        _ = random.seed(seed)\n",
    "        _ = np.random.seed(seed)\n",
    "        _ = torch.manual_seed(seed)\n",
    "        if test_split == 'free_test':\n",
    "            free_eval_df = free_test_df\n",
    "        else:\n",
    "            free_eval_df = free_gen_df\n",
    "        run_name = f\"gpt3.{engine}.nshot.{nshot}.split.{test_split}.seed.{seed}\"\n",
    "        print(f\"RUNNING: {run_name}\")\n",
    "        # eval\n",
    "        _ = run_gpt3_experiment(\n",
    "            run_name, \n",
    "            free_train_df, \n",
    "            free_eval_df.sample(LIMIT_TEST_N), \n",
    "            nshot=nshot, \n",
    "            engine=engine, \n",
    "            start_prompt=True, \n",
    "            temperature=0.0, \n",
    "            batch_size=20,\n",
    "        )\n",
    "        print(f\"ANALYZING: {run_name}\")\n",
    "        output_parent_dir = \"openai-results\"\n",
    "        output_filename = f\"./{output_parent_dir}/{run_name}.json\"\n",
    "        output = json.load(open(output_filename))\n",
    "        total_count = 0\n",
    "        correct_count = 0\n",
    "        for ex in output:\n",
    "            total_count += 1\n",
    "            if ex[\"target_answer\"] == ex[\"generated_answer\"]:\n",
    "                correct_count += 1\n",
    "        exact_match = correct_count/total_count\n",
    "        print(f\"Exact Match: {exact_match}\")\n",
    "        free_results += [[engine, test_split, seed, exact_match]]\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
