{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import six \n",
    "\n",
    "def convert_tokens_to_ids(vocab, tokens):\n",
    "    \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n",
    "    ids = []\n",
    "    for token in tokens:\n",
    "        if token not in vocab.keys():\n",
    "            ids.append(vocab['[UNK]'])\n",
    "        else:\n",
    "            ids.append(vocab[token])\n",
    "    return ids\n",
    "def convert_to_unicode(text):\n",
    "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "    if six.PY3:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, bytes):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    elif six.PY2:\n",
    "        if isinstance(text, str):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        elif isinstance(text, unicode):\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    else:\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")\n",
    "    \n",
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = collections.OrderedDict()\n",
    "    index = 0\n",
    "    with open(vocab_file, \"r\") as reader:\n",
    "        while True:\n",
    "            token = convert_to_unicode(reader.readline())\n",
    "            if not token:\n",
    "                break\n",
    "            token = token.strip()\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "    return vocab\n",
    "class WordLevelTokenizer(object):\n",
    "    \"\"\"Runs end-to-end tokenziation.\"\"\"\n",
    "    def __init__(self, vocab_file, config, delimiter=\" \", max_seq_len=128):\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.vocab_reverse = collections.OrderedDict()\n",
    "        for k, v in self.vocab.items():\n",
    "            self.vocab_reverse[v] = k\n",
    "        self.pad_token_id = config.pad_token_id\n",
    "        self.bos_token_id = config.bos_token_id\n",
    "        self.eos_token_id = config.eos_token_id\n",
    "        self.unk_token_id = config.unk_token_id\n",
    "        self.mask_token_id = config.mask_token_id\n",
    "        self.special_token_ids = set(\n",
    "            [config.pad_token_id, config.bos_token_id, config.eos_token_id, \n",
    "            config.unk_token_id, config.mask_token_id]\n",
    "        )\n",
    "        \n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.delimiter = delimiter\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        split_tokens = []\n",
    "        for token in text.split(self.delimiter):\n",
    "            split_tokens.append(token)\n",
    "        return split_tokens\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return convert_tokens_to_ids(self.vocab, tokens)\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        original = self.convert_tokens_to_ids(self.tokenize(text))\n",
    "        original = original[:(self.max_seq_len-2)]\n",
    "        return [self.bos_token_id] + original + [self.eos_token_id]\n",
    "    \n",
    "    def batch_decode(self, pred_labels, skip_special_tokens=True):\n",
    "        decode_labels_batch = []\n",
    "        for labels in pred_labels:\n",
    "            decode_labels = []\n",
    "            for l in labels.tolist():\n",
    "                if l == self.eos_token_id:\n",
    "                    break\n",
    "                if l not in self.special_token_ids:\n",
    "                    decode_labels += [self.vocab_reverse[l]]\n",
    "            decode_labels_batch += [self.delimiter.join(decode_labels)]\n",
    "        return decode_labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoConfig\n",
    "import torch\n",
    "from torch import Tensor \n",
    "from jaxtyping import Shaped\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 67\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config_encoder = AutoConfig.from_pretrained(\n",
    "                os.path.join(\"/Users/georgenakayama/Documents/Academics/CS/CS224U/ReCOGS_224U/model\", \"encoder_config.json\")\n",
    "            )\n",
    "\n",
    "config_decoder = AutoConfig.from_pretrained(\n",
    "                    os.path.join(\"/Users/georgenakayama/Documents/Academics/CS/CS224U/ReCOGS_224U/model\", \"decoder_config.json\")\n",
    "            )\n",
    "src_tokenizer = WordLevelTokenizer(\n",
    "    os.path.join(\"/Users/georgenakayama/Documents/Academics/CS/CS224U/ReCOGS_224U/model\", \"src_vocab.txt\"), \n",
    "    config_encoder,\n",
    "    max_seq_len=512\n",
    ")\n",
    "tgt_tokenizer = WordLevelTokenizer(\n",
    "    os.path.join(\"/Users/georgenakayama/Documents/Academics/CS/CS224U/ReCOGS_224U/model\", \"tgt_vocab.txt\"), \n",
    "    config_decoder,\n",
    "    max_seq_len=512\n",
    ")\n",
    "AND_token_id = tgt_tokenizer(\"AND\")[1]\n",
    "SEMICOLON_token_id = tgt_tokenizer(\";\")[1]\n",
    "print(AND_token, SEMICOLON_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE permuting the ground truth tokens\n",
      "naive cross entropy tensor(7.0860) \n",
      " left sum chamfer cross entropy tensor(6.9762) \n",
      " right sum chamfer cross entropy tensor(6.9762) \n",
      "\n",
      "AFTER permuting the ground truth tokens\n",
      "naive cross entropy tensor(7.0904) \n",
      " left sum chamfer cross entropy tensor(6.9762) \n",
      " right sum chamfer cross entropy tensor(6.9762) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def chamferToken(loss_fn: Callable , \n",
    "                 a: Shaped[Tensor, \"bs nt nl vs\"], \n",
    "                 b: Shaped[Tensor, \"bs nt nl\"], \n",
    "                 mask_a: Shaped[Tensor, \"bs nt nl\"], \n",
    "                 mask_b: Shaped[Tensor, \"bs nt nl\"],\n",
    "                 reduce: bool=True,\n",
    "                 split=False):\n",
    "    \"\"\"\n",
    "    chamfer dist for tokens\n",
    "    loss_fn: loss function that computes distance between tokens\n",
    "    a: [bs, nt, nl, vocab_size] number of tokens, max length\n",
    "    b: [bs, nt, nl]\n",
    "    mask_a, mask_b: [bs, nt, nl], mask of valid tokens\n",
    "    reduce: if reduce, will return the averaged number\n",
    "    NOTE: loss_fn should NOT output averaged elements, and should NOT ignore indices. \n",
    "    ignored indices should be indicated in masks\n",
    "    \"\"\"\n",
    "    bs, nt, nl, vs = a.shape\n",
    "    # for each token in a, get the min dist in b  \n",
    "    token_mask_a, token_mask_b = mask_a.sum(-1) == nl, mask_b.sum(-1) == nl\n",
    "    num_unmasked_a, num_unmasked_b = nl - mask_a.sum(-1), nl - mask_b.sum(-1) \n",
    "    num_unmasked_a = torch.where(num_unmasked_a == 0, nl, num_unmasked_a)\n",
    "    num_unmasked_b = torch.where(num_unmasked_b == 0, nl, num_unmasked_b)\n",
    "    _a = a.unsqueeze(2).repeat_interleave(nt, dim=2).permute(0, 1, 4, 2, 3).reshape(bs*nt, vs, nt, nl) # [bs, nt, vs, nt, nl]\n",
    "    _b = b.unsqueeze(1).repeat_interleave(nt, dim=1).reshape(bs*nt, nt, nl) #[bs, nt, nt, nl]\n",
    "    a_loss = loss_fn(_a, _b).reshape(bs, nt, nt, nl) # [bs* nt, nt, nl]\n",
    "    a_loss = a_loss.masked_fill(mask_b.reshape(bs, 1, nt, nl), value=0).sum(-1) / num_unmasked_b.unsqueeze(1)\n",
    "    a_loss = a_loss.masked_fill(token_mask_b.unsqueeze(1), value=1e9)\n",
    "    a_loss = a_loss.min(-1)[0] # [bs, nt]\n",
    "    a_loss = a_loss.masked_fill(token_mask_a, value=0)\n",
    "    if reduce:\n",
    "        a_loss = a_loss.sum(-1) / (nt - token_mask_a.sum(-1))\n",
    "    \n",
    "    _a = a.unsqueeze(1).repeat_interleave(nt, dim=1).permute(0, 1, 4, 2, 3).reshape(bs*nt, vs, nt, nl) # [bs, nt, vs, nt, nl]\n",
    "    _b = b.unsqueeze(2).repeat_interleave(nt, dim=2).reshape(bs*nt, nt, nl) #[bs, nt, nt, nl]\n",
    "    b_loss = loss_fn(_a, _b).reshape(bs, nt, nt, nl) # [bs* nt, nt, nl]\n",
    "    b_loss = b_loss.masked_fill(mask_a.reshape(bs, 1, nt, nl), value=0).sum(-1) / num_unmasked_a.unsqueeze(1)\n",
    "    b_loss = b_loss.masked_fill(token_mask_a.unsqueeze(1), value=1e9)\n",
    "    b_loss = b_loss.min(-1)[0] # [bs, nt]\n",
    "    b_loss = b_loss.masked_fill(token_mask_b, value=0)\n",
    "    if reduce:\n",
    "        b_loss = b_loss.sum(-1) / (nt - token_mask_b.sum(-1))\n",
    "    \n",
    "    if split:\n",
    "        return a_loss.mean(), b_loss.mean()\n",
    "    loss = a_loss + b_loss \n",
    "    if reduce:\n",
    "        return loss.mean()\n",
    "    return loss\n",
    "\n",
    "bs, nt, nl, vs = 125, 5, 108, 729\n",
    "# ids = torch.randint(0, vs, size=(bs*nt*nl,))\n",
    "# a = torch.eye(vs)[ids].reshape(bs, nt, nl, vs)\n",
    "# b = ids.reshape(bs, nt, nl)\n",
    "# b = b[:,torch.randperm(nt)]\n",
    "# a = a[:, :,torch.randperm(nt)]\n",
    "# print(b.shape)\n",
    "# loss_func = lambda x, y: (x.argmax(1) - y) ** 2\n",
    "# mask_a = torch.zeros_like(b).bool()\n",
    "# mask_b = torch.zeros_like(b).bool()\n",
    "# loss = chamferToken(loss_func, a, b, mask_a, mask_b)\n",
    "# print(loss)\n",
    "\n",
    "def distChamfer(a, b):\n",
    "    \n",
    "    x, y = a, b\n",
    "    bs, num_points, points_dim = x.size()\n",
    "    xx = torch.bmm(x, x.transpose(2, 1))\n",
    "    yy = torch.bmm(y, y.transpose(2, 1))\n",
    "    zz = torch.bmm(x, y.transpose(2, 1))\n",
    "    diag_ind = torch.arange(0, num_points).to(a).long()\n",
    "    rx = xx[:, diag_ind, diag_ind].unsqueeze(1).expand_as(xx)\n",
    "    ry = yy[:, diag_ind, diag_ind].unsqueeze(1).expand_as(yy)\n",
    "    P = (rx.transpose(2, 1) + ry - 2 * zz)\n",
    "    return P.min(1)[0], P.min(2)[0]\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "a = torch.randn(bs, nt, nl, vs)\n",
    "b = torch.randint(0, vs, size=(bs, nt, nl))\n",
    "_b = b[:, torch.randperm(nt)]\n",
    "a_mask = torch.zeros_like(b)\n",
    "b_mask = torch.zeros_like(b)\n",
    "def foo(a, b):\n",
    "    return F.cross_entropy(a, b, reduction=\"none\")\n",
    "l, r = chamferToken(foo, a, b, a_mask, b_mask, split=True)\n",
    "l_permed, r_permed = chamferToken(foo, a, _b, a_mask, b_mask, split=True)\n",
    "gt = F.cross_entropy(a.permute(0, 3, 1, 2), b)\n",
    "gt_permed = F.cross_entropy(a.permute(0, 3, 1, 2), _b)\n",
    "print(\"BEFORE permuting the ground truth tokens\")\n",
    "print(\"naive cross entropy\", gt, \"\\n\", \"left sum chamfer cross entropy\" ,l,  \"\\n\", \"right sum chamfer cross entropy\",r,  \"\\n\")\n",
    "print(\"AFTER permuting the ground truth tokens\")\n",
    "print(\"naive cross entropy\", gt_permed, \"\\n\", \"left sum chamfer cross entropy\" ,l_permed,  \"\\n\", \"right sum chamfer cross entropy\",r_permed,  \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/georgenakayama/Documents/Academics/CS/CS224U/ReCOGS_224U/cogs_token_removal/test_remove_x_.tsv\"\n",
    "items = []\n",
    "eval_cat = []\n",
    "for l in open(path, \"r\").readlines():\n",
    "    text, sparse, cat = l.split(\"\\t\")\n",
    "    src_input_ids = src_tokenizer(text)\n",
    "    tgt_input_ids = tgt_tokenizer(sparse)\n",
    "    try:\n",
    "        semicolon_id = len(tgt_input_ids) - tgt_input_ids[::-1].index(SEMICOLON_token_id) - 1\n",
    "    except ValueError:\n",
    "        semicolon_id = -1\n",
    "    print(semicolon_id)\n",
    "    conj_sparse = torch.tensor(tgt_input_ids[semicolon_id + 1:])\n",
    "    AND_mask = conj_sparse == AND_token_id\n",
    "    ids = torch.eye(conj_sparse.shape[0])[AND_mask].argmax(-1)\n",
    "    ids = torch.cat([torch.zeros(1), ids, torch.ones(1) * conj_sparse.shape[0] - 1])\n",
    "    conj_lens = ids[1:] - ids[:-1]\n",
    "    print(ids)\n",
    "    print(conj_lens)\n",
    "    conjs = sparse.split(\";\")[-1].split(\"AND\")\n",
    "    print(src_input_ids, tgt_input_ids)\n",
    "    print(text)\n",
    "    print(sparse)\n",
    "    print(conjs)\n",
    "    print(cat.strip())\n",
    "    print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs228",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
