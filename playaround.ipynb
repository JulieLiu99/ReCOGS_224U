{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import six \n",
    "\n",
    "def convert_tokens_to_ids(vocab, tokens):\n",
    "    \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n",
    "    ids = []\n",
    "    for token in tokens:\n",
    "        if token not in vocab.keys():\n",
    "            ids.append(vocab['[UNK]'])\n",
    "        else:\n",
    "            ids.append(vocab[token])\n",
    "    return ids\n",
    "def convert_to_unicode(text):\n",
    "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "    if six.PY3:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, bytes):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    elif six.PY2:\n",
    "        if isinstance(text, str):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        elif isinstance(text, unicode):\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    else:\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")\n",
    "    \n",
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = collections.OrderedDict()\n",
    "    index = 0\n",
    "    with open(vocab_file, \"r\") as reader:\n",
    "        while True:\n",
    "            token = convert_to_unicode(reader.readline())\n",
    "            if not token:\n",
    "                break\n",
    "            token = token.strip()\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "    return vocab\n",
    "class WordLevelTokenizer(object):\n",
    "    \"\"\"Runs end-to-end tokenziation.\"\"\"\n",
    "    def __init__(self, vocab_file, config, delimiter=\" \", max_seq_len=128):\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.vocab_reverse = collections.OrderedDict()\n",
    "        for k, v in self.vocab.items():\n",
    "            self.vocab_reverse[v] = k\n",
    "        self.pad_token_id = config.pad_token_id\n",
    "        self.bos_token_id = config.bos_token_id\n",
    "        self.eos_token_id = config.eos_token_id\n",
    "        self.unk_token_id = config.unk_token_id\n",
    "        self.mask_token_id = config.mask_token_id\n",
    "        self.special_token_ids = set(\n",
    "            [config.pad_token_id, config.bos_token_id, config.eos_token_id, \n",
    "            config.unk_token_id, config.mask_token_id]\n",
    "        )\n",
    "        \n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.delimiter = delimiter\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        split_tokens = []\n",
    "        for token in text.split(self.delimiter):\n",
    "            split_tokens.append(token)\n",
    "        return split_tokens\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return convert_tokens_to_ids(self.vocab, tokens)\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        original = self.convert_tokens_to_ids(self.tokenize(text))\n",
    "        original = original[:(self.max_seq_len-2)]\n",
    "        return [self.bos_token_id] + original + [self.eos_token_id]\n",
    "    \n",
    "    def batch_decode(self, pred_labels, skip_special_tokens=True):\n",
    "        decode_labels_batch = []\n",
    "        for labels in pred_labels:\n",
    "            decode_labels = []\n",
    "            for l in labels.tolist():\n",
    "                if l == self.eos_token_id:\n",
    "                    break\n",
    "                if l not in self.special_token_ids:\n",
    "                    decode_labels += [self.vocab_reverse[l]]\n",
    "            decode_labels_batch += [self.delimiter.join(decode_labels)]\n",
    "        return decode_labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoConfig\n",
    "import torch\n",
    "from torch import Tensor \n",
    "from jaxtyping import Shaped\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 67\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config_encoder = AutoConfig.from_pretrained(\n",
    "                os.path.join(\"/Users/georgenakayama/Documents/Academics/CS/CS224U/ReCOGS_224U/model\", \"encoder_config.json\")\n",
    "            )\n",
    "\n",
    "config_decoder = AutoConfig.from_pretrained(\n",
    "                    os.path.join(\"/Users/georgenakayama/Documents/Academics/CS/CS224U/ReCOGS_224U/model\", \"decoder_config.json\")\n",
    "            )\n",
    "src_tokenizer = WordLevelTokenizer(\n",
    "    os.path.join(\"/Users/georgenakayama/Documents/Academics/CS/CS224U/ReCOGS_224U/model\", \"src_vocab.txt\"), \n",
    "    config_encoder,\n",
    "    max_seq_len=512\n",
    ")\n",
    "tgt_tokenizer = WordLevelTokenizer(\n",
    "    os.path.join(\"/Users/georgenakayama/Documents/Academics/CS/CS224U/ReCOGS_224U/model\", \"tgt_vocab.txt\"), \n",
    "    config_decoder,\n",
    "    max_seq_len=512\n",
    ")\n",
    "AND_token_id = tgt_tokenizer(\"AND\")[1]\n",
    "SEMICOLON_token_id = tgt_tokenizer(\";\")[1]\n",
    "print(AND_token, SEMICOLON_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "def chamferToken(loss_fn: Callable , \n",
    "                 a: Shaped[Tensor, \"bs nt nl vs\"], \n",
    "                 b: Shaped[Tensor, \"bs nt nl\"], \n",
    "                 mask_a: Shaped[Tensor, \"bs nt nl\"], \n",
    "                 mask_b: Shaped[Tensor, \"bs nt nl\"],\n",
    "                 reduce: bool=True):\n",
    "    \"\"\"\n",
    "    chamfer dist for tokens\n",
    "    loss_fn: loss function that computes distance between tokens\n",
    "    a: [bs, nt, nl, vocab_size] number of tokens, max length\n",
    "    b: [bs, nt, nl]\n",
    "    mask_a, mask_b: [bs, nt, nl], mask of valid tokens\n",
    "    reduce: if reduce, will return the averaged number\n",
    "    NOTE: loss_fn should NOT output averaged elements, and should NOT ignore indices. \n",
    "    ignored indices should be indicated in masks\n",
    "    \"\"\"\n",
    "    bs, nt, nl, vs = a.shape\n",
    "    # for each token in a, get the min dist in b  \n",
    "    token_mask_a, token_mask_b = mask_a.sum(-1) == nl, mask_b.sum(-1) == nl\n",
    "    num_unmasked_a, num_unmasked_b = nl - mask_a.sum(-1), nl - mask_b.sum(-1) \n",
    "    num_unmasked_a = torch.where(num_unmasked_a == 0, nl, num_unmasked_a)\n",
    "    num_unmasked_b = torch.where(num_unmasked_b == 0, nl, num_unmasked_b)\n",
    "    _a = a.unsqueeze(2).repeat_interleave(nt, dim=2).permute(0, 1, 4, 2, 3).reshape(bs*nt, vs, nt, nl) # [bs, nt, vs, nt, nl]\n",
    "    _b = b.unsqueeze(1).repeat_interleave(nt, dim=1).reshape(bs*nt, nt, nl) #[bs, nt, nt, nl]\n",
    "    a_loss = loss_fn(_a, _b).reshape(bs, nt, nt, nl) # [bs* nt, nt, nl]\n",
    "    a_loss = a_loss.masked_fill(mask_b.reshape(bs, 1, nt, nl), value=0).sum(-1) / num_unmasked_b.unsqueeze(1)\n",
    "    a_loss = a_loss.masked_fill(token_mask_b.unsqueeze(1), value=1e9)\n",
    "    a_loss = a_loss.min(-1)[0] # [bs, nt]\n",
    "    a_loss = a_loss.masked_fill(token_mask_a, value=0)\n",
    "    if reduce:\n",
    "        a_loss = a_loss.sum(-1) / (nt - token_mask_a.sum(-1))\n",
    "    \n",
    "    _a = a.unsqueeze(1).repeat_interleave(nt, dim=1).permute(0, 1, 4, 2, 3).reshape(bs*nt, vs, nt, nl) # [bs, nt, vs, nt, nl]\n",
    "    _b = b.unsqueeze(2).repeat_interleave(nt, dim=2).reshape(bs*nt, nt, nl) #[bs, nt, nt, nl]\n",
    "    b_loss = loss_fn(_a, _b).reshape(bs, nt, nt, nl) # [bs* nt, nt, nl]\n",
    "    b_loss = b_loss.masked_fill(mask_a.reshape(bs, 1, nt, nl), value=0).sum(-1) / num_unmasked_a.unsqueeze(1)\n",
    "    b_loss = b_loss.masked_fill(token_mask_a.unsqueeze(1), value=1e9)\n",
    "    b_loss = b_loss.min(-1)[0] # [bs, nt]\n",
    "    b_loss = b_loss.masked_fill(token_mask_b, value=0)\n",
    "    if reduce:\n",
    "        b_loss = b_loss.sum(-1) / (nt - token_mask_b.sum(-1))\n",
    "        \n",
    "    loss = a_loss + b_loss \n",
    "    if reduce:\n",
    "        return loss.mean()\n",
    "    return loss\n",
    "    \n",
    "bs, nt, nl, vs = 2, 3, 4, 5\n",
    "ids = torch.randint(0, vs, size=(bs*nt*nl,))\n",
    "a = torch.eye(vs)[ids].reshape(bs, nt, nl, vs)\n",
    "b = ids.reshape(bs, nt, nl)\n",
    "loss_func = lambda x, y: (x.argmax(1) - y) ** 2\n",
    "mask_a = torch.zeros_like(b).bool()\n",
    "mask_b = torch.zeros_like(b).bool()\n",
    "loss = chamferToken(loss_func, a, b, mask_a, mask_b)\n",
    "print(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/georgenakayama/Documents/Academics/CS/CS224U/ReCOGS_224U/cogs_token_removal/test_remove_x_.tsv\"\n",
    "items = []\n",
    "eval_cat = []\n",
    "for l in open(path, \"r\").readlines():\n",
    "    text, sparse, cat = l.split(\"\\t\")\n",
    "    src_input_ids = src_tokenizer(text)\n",
    "    tgt_input_ids = tgt_tokenizer(sparse)\n",
    "    try:\n",
    "        semicolon_id = len(tgt_input_ids) - tgt_input_ids[::-1].index(SEMICOLON_token_id) - 1\n",
    "    except ValueError:\n",
    "        semicolon_id = -1\n",
    "    print(semicolon_id)\n",
    "    conj_sparse = torch.tensor(tgt_input_ids[semicolon_id + 1:])\n",
    "    AND_mask = conj_sparse == AND_token_id\n",
    "    ids = torch.eye(conj_sparse.shape[0])[AND_mask].argmax(-1)\n",
    "    ids = torch.cat([torch.zeros(1), ids, torch.ones(1) * conj_sparse.shape[0] - 1])\n",
    "    conj_lens = ids[1:] - ids[:-1]\n",
    "    print(ids)\n",
    "    print(conj_lens)\n",
    "    conjs = sparse.split(\";\")[-1].split(\"AND\")\n",
    "    print(src_input_ids, tgt_input_ids)\n",
    "    print(text)\n",
    "    print(sparse)\n",
    "    print(conjs)\n",
    "    print(cat.strip())\n",
    "    print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs228",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
